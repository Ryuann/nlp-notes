[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "01_llms/lecture1.html#reading-eight-things-to-know-about-large-language-models",
    "href": "01_llms/lecture1.html#reading-eight-things-to-know-about-large-language-models",
    "title": "Lecture 1",
    "section": "Reading: Eight Things to Know about Large Language Models",
    "text": "Reading: Eight Things to Know about Large Language Models\n\nLLMs predictably get more capable with increasing investment, even without targeted innovation.\n\nScaling laws：只要增加训练数据、模型参数和计算资源，语言模型的能力通常会逐渐变强，这种增强是可预测的（通过数学模型）。\nGPT、GPT-2和GPT-3的区别主要来自于训练规模的巨大差异\n\nMany important LLM behaviors emerge unpredictably as a byproduct of increasing investment.\n\n某些复杂能力不是逐步提高的，而是在模型达到某个规模“临界点”后突然显现。例如GPT-3 显示出“少样本学习”（few-shot learning）能力。\n\nLLMs often appear to learn and use representations of the outside world.\n\n为了更好地预测下一个词，模型“脑中”形成了某种结构化的世界理解\n\nThere are no reliable techniques for steering the behavior of LLMs.\nExperts are not yet able to interpret the inner workings of LLMs.\nHuman performance on a task isn’t an upper bound on LLM performance.\nLLMs need not express the values of their creators nor the values encoded in web text.\nBrief interactions with LLMs are often misleading."
  },
  {
    "objectID": "01_llms/lecture1.html#llm-stats",
    "href": "01_llms/lecture1.html#llm-stats",
    "title": "Lecture 1",
    "section": "LLM Stats",
    "text": "LLM Stats\nLLM Stats 是一个实时追踪和可视化大型语言模型发展的网站，涵盖模型发布时间线、参数量、开源/闭源状态、评估表现、碳排放等。\n\n快速对比不同模型（如 GPT-4、Claude、LLaMA、Gemini）的能力；\n了解当前主流评测榜单（如 MMLU, BIG-Bench, HumanEval）上的表现；\n掌握大模型增长趋势及背后的推理代价（算力与排放）； & 判断一个模型是否适合某研究或应用需求。"
  },
  {
    "objectID": "01_llms/lecture1.html#capabilities",
    "href": "01_llms/lecture1.html#capabilities",
    "title": "Lecture 1",
    "section": "Capabilities",
    "text": "Capabilities\n\nPerform professional and academic exams at normal intelligence human\n\n包括 LSAT、GRE、SAT、USMLE 等\n\nLearns a game from a move on a board\n\nfew-shot learning + in-context learning → 模型无需再训练即可理解任务\n\nWrites and debugs computer code\n\n语料包含 GitHub、StackOverflow 等，衍生出 Codex、CodeGen、StarCoder\n\nTags texts (often) better than human coders\n\n人机协同标注效果最佳\n\nCan create, adjust, tag and describe images\n\n多模态模型：CLIP, DALL·E, GPT-4V\n\nIs able to complete complex tasks, especially with chain-of-thought / least-to-most prompting"
  },
  {
    "objectID": "01_llms/lecture1.html#process传统完整阶段",
    "href": "01_llms/lecture1.html#process传统完整阶段",
    "title": "Lecture 1",
    "section": "Process（传统完整阶段）",
    "text": "Process（传统完整阶段）\n\nBefore Preprocessing\n\nCreate dictionaries\n\nTagging / annotation\n\n\n\nLexical Analysis\n\nBi-grams / N-grams\n\nPOS tagging\n\nParse tree construction\n\n\n\nPrediction & Reporting\n\nConcept extraction (e.g., LDA)\n\nSentiment analysis\n\nPredictive analytics\n\nVisualization"
  },
  {
    "objectID": "01_llms/lecture1.html#glue",
    "href": "01_llms/lecture1.html#glue",
    "title": "Lecture 1",
    "section": "GLUE",
    "text": "GLUE\n\nMNLI、RTE、STS-B、SST-2、QQP、CoLA 等"
  },
  {
    "objectID": "01_llms/lecture1.html#squad-1.1-2.0",
    "href": "01_llms/lecture1.html#squad-1.1-2.0",
    "title": "Lecture 1",
    "section": "SQuAD 1.1 / 2.0",
    "text": "SQuAD 1.1 / 2.0\n\n问答系统标准数据集"
  },
  {
    "objectID": "01_llms/lecture1.html#swag",
    "href": "01_llms/lecture1.html#swag",
    "title": "Lecture 1",
    "section": "SWAG",
    "text": "SWAG\n\n常识推理 + 对抗选项"
  },
  {
    "objectID": "01_llms/lecture1.html#big-bench",
    "href": "01_llms/lecture1.html#big-bench",
    "title": "Lecture 1",
    "section": "BIG-bench",
    "text": "BIG-bench\n\nGoogle 出品，覆盖数学、逻辑、模仿、代码等 200+ 任务\n\n涉及“涌现能力”评估"
  },
  {
    "objectID": "01_llms/lecture1.html#mme",
    "href": "01_llms/lecture1.html#mme",
    "title": "Lecture 1",
    "section": "MME",
    "text": "MME\n\n多模态评估标准，测试文本+图像理解任务"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#language-modeling-and-probability-foundations",
    "href": "02_topic_sentiment/lecture2.html#language-modeling-and-probability-foundations",
    "title": "Lecture 2",
    "section": "Language Modeling and Probability Foundations",
    "text": "Language Modeling and Probability Foundations\n\nSequence Modeling and Prediction\nLanguage models predict the next word in a sequence given the preceding words. This is typically modeled using conditional probability:\n语言模型的基本任务是：根据前面的词来预测下一个词。用Conditional Probability表示：\n\\[\nP(w_5 \\mid w_1, w_2, w_3, w_4)\n\\]\nThis expresses the probability of the 5th word given the first four.\n\nExample:\nSentence: “Luck is what happens when preparation meets opportunity.”\n在“Luck is what happens”之后出现“when”的概率是多少？\n\n\n\nChain Rule of Probability\nTo compute complex conditional probabilities, we use the chain rule:\n为了计算复杂的条件概率，我们使用概率的Chain Rule：\n\nBasic rule: \\(P(A \\cap B) = P(A) P(B|A)\\)\nConditional probability definition: \\(P(B|A) = \\frac{P(A \\cap B)}{P(A)}\\)\nGeneralized form: \\(P(A \\cap B \\cap C) = P(C|B \\cap A) P(B|A) P(A)\\)\n\nIn the context of language:\n目标是估计：\n\\[\nP(\\text{when} \\mid \\text{happens}, \\text{what}, \\text{is}, \\text{luck})\n\\]\n使用chain rule展开：\n\\[\nP(\\text{when}) = P(\\text{when} \\mid \\text{happens} \\cap \\text{what} \\cap \\text{is} \\cap \\text{luck}) \\\\\n\\cdot P(\\text{happens} \\mid \\text{what} \\cap \\text{is} \\cap \\text{luck}) \\\\\n\\cdot P(\\text{what} \\mid \\text{is} \\cap \\text{luck}) \\\\\n\\cdot P(\\text{is} \\mid \\text{luck}) \\\\\n\\cdot P(\\text{luck})\n\\]\n这就是语言模型中常用的思路：将一个句子中各个词的联合概率转化为一系列条件概率的乘积。"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#perplexity-and-cross-entropy",
    "href": "02_topic_sentiment/lecture2.html#perplexity-and-cross-entropy",
    "title": "Lecture 2",
    "section": "Perplexity and Cross-Entropy",
    "text": "Perplexity and Cross-Entropy\n\nPerplexity\nPerplexity evaluates how well a probability model predicts a sample:\n\nPerplexity = 2 raised to the cross-entropy of the model\n\nPerplexity衡量的是模型对测试集的“困惑”程度，越小越好。\n它等价于模型每预测一个词时的不确定度。\n\\[\n\\text{Perplexity} = 2^{-\\frac{1}{W} \\sum_{k=1}^{n} \\log P(w_k)}\n\\]\n其中：\n\n\\(W\\)：测试集中单词总数\n\\(P(w_k)\\)：模型给第 \\(k\\) 个词的概率预测\n\n\n\nCross-Entropy\nCross-entropy quantifies the average number of bits needed to encode data from a distribution \\(\\tilde{p}\\) using a model \\(q\\):\n交叉熵（Cross-Entropy）用来衡量一个分布 \\(\\tilde{p}\\) 和模型分布 \\(q\\) 之间的距离：\n\\[\nH(\\tilde{p}, q) = -\\sum_{i=1}^N \\tilde{p}(x_i) \\log_2 q(x_i)\n\\]\n其中：\n\n\\(\\tilde{p}(x_i)\\)：经验概率（词 \\(x_i\\) 的出现频率）\n\\(q(x_i)\\)：模型预测的概率\n\n和 Perplexity 的关系：\n\\[\n\\text{Perplexity} = 2^{\\text{Cross-Entropy}}\n\\]\n\n\nKL Divergence (Relative Entropy)\nKL 散度衡量一个近似分布 \\(q\\) 与真实分布 \\(p\\) 的差异。它不对称，但如果两者相等，KL 散度为 0。\n公式：\n\\[\nD_{KL}(p \\parallel q) = \\sum_{i=1}^N p(x_i) \\log \\left( \\frac{p(x_i)}{q(x_i)} \\right)\n\\]\n\n\\(p(x_i)\\)：真实分布\n\\(q(x_i)\\)：模型的近似分布\n\n用途：KL 散度在训练过程中常用于优化目标函数，比如通过最小化 KL 散度来逼近真实分布。"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#text-representation-models",
    "href": "02_topic_sentiment/lecture2.html#text-representation-models",
    "title": "Lecture 2",
    "section": "Text Representation Models",
    "text": "Text Representation Models\n\nBag-of-Words (BoW)\nConverts text into vectors based on word occurrence counts.\nBoW（词袋模型）将文本向量化，每个维度表示词表中一个词在句子中出现的频率。\n\n例子说明：\n句子：\nI WOULD NOT, COULD NOT IN THE RAIN.\nNOT IN THE DARK. NOT ON A TRAIN.\n被转换为两个向量：\n[1, 1, 1, 2, 1, 1, 1, 1, 0, 0, ..., 0]\n[0, 0, 0, 2, 1, 1, 0, 1, 1, 1, ..., 0]\n每个数表示词表中对应单词在该“文档”（即句子）中出现的次数。\n\n\n\nLimitations of BoW\nBoW 的主要缺点：\n\n不考虑语义（Semantic meaning）：忽略上下文，比如“我爱你”和“你爱我”会被视为相同向量。\n维度高（Vector size）：词表大时，向量稀疏、占用资源多。\n\n\n\nTF-IDF (Term Frequency-Inverse Document Frequency)\nGives less importance to common words:\n目的：降低高频无意义词（如 “the”, “and”）的权重，提高有区分度的词的重要性。\n定义：\n\nTF（Term Frequency）：词在文档中的频率；\nIDF（Inverse Document Frequency）：词在整个文档集中出现的“逆频率”。\n\n公式：\n\\[\n\\text{TF-IDF}(t, d) = tf(t, d) \\cdot \\log \\left( \\frac{N}{df(t)} \\right)\n\\]\n其中：\n\n\\(t\\)：词\n\\(d\\)：文档\n\\(df(t)\\)：包含该词的文档数\n\\(N\\)：文档总数\n\n\n\nNormalized Term Frequency\n为了解决文档长短对词频带来的偏差，可以进行规范化处理：\n\n对数缩放（Log Normalization）： \\(\\log(1 + f_{t,d})\\)\n最大值归一化（Max Normalization）：\n\\[\ntf(t,d) = 0.5 + 0.5 \\cdot \\frac{f_{t,d}}{\\max \\{f_{t',d} : t' \\in d\\}}\n\\]"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#n-gram-models",
    "href": "02_topic_sentiment/lecture2.html#n-gram-models",
    "title": "Lecture 2",
    "section": "N-gram Models",
    "text": "N-gram Models\n\nBi-grams and N-grams\nBi-gram（双词模型）解决 BoW 忽略词序的问题，将每两个词作为一个单元进行建模。\n例如：\n\"Luck is\", \"is what\", \"what happens\", ...\n\n\nN-gram 一般化\nN-gram 是对 Bi-gram 的拓展，使用前 \\(N-1\\) 个词预测当前词。\n\n数学表达式：\n\nUnigram: \\(P(w_i) = \\frac{\\text{Count}(w_i)}{\\sum_j \\text{Count}(w_j)}\\)\nBigram: \\(P(w_i | w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}\\)\nN-gram: \\(P(w_1^n) = \\prod_{k=1}^{n} P(w_k | w_{k-1})\\)\n\n表示整句话的概率是各个词的条件概率连乘。\n\n\n\nOut-of-Vocabulary Words（词表外词）\n当训练集中没有出现的词叫 OOV（Out-of-Vocabulary），这会导致概率为零。\n解决办法：\n\n扩大语料库（Increase corpus size）\n跳过缺失的 n-gram（Leap over missing n-grams）\n使用平滑技术（Smoothing），如： \\(P_{\\text{Laplace}}(w_i) = \\frac{c_i + 1}{N + V}\\) 其中：\n\n\\(c_i\\)：词 \\(w_i\\) 出现次数\n\\(N\\)：词总数\n\\(V\\)：词表大小"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#sentiment-analysis-and-semantic-orientation",
    "href": "02_topic_sentiment/lecture2.html#sentiment-analysis-and-semantic-orientation",
    "title": "Lecture 2",
    "section": "Sentiment Analysis and Semantic Orientation",
    "text": "Sentiment Analysis and Semantic Orientation\n\nDefinition\nSemantic orientation analyzes sentiment strength and direction in text.\n语义倾向（Semantic Orientation）用于判断文本中单词或短语的情绪极性和强度。\n\nKey dimensions:\n\nSubjectivity: objective vs subjective （客观 vs 主观）\nPolarity: negative vs positive （负面 vs 正面）\nIntensity: strength (e.g. “very good” vs “good”)\n\n\n\n\nLexicon-Based Approach\n基于人工或半自动构建的情感词典，如 Hu & Liu（2004）的方法：\n\n提取 意见词（opinion words），如形容词（good, bad, amazing）；\n使用 WordNet 获取同义词/反义词，推断其情绪倾向；\n逐词分析并合并为句子级别判断。\n\n\n\nWordNet Role\nWordNet 是一个词汇知识库，它用语义关系连接单词，包括：\n\nSynonym（同义词）：形成同义集 Synsets\nAntonym（反义词）：如 wet 与 dry\n上下位关系（Hypernym-Hyponym）：如 furniture &gt; bed\n部分整体关系（Meronym）：如 chair 与 backrest\n\n\n\nLimitations of Lexicon Methods\n引用自 Agarwal & Mittal（2015）：\n主要问题：\n\n依赖训练语料库中的已有词汇；\n对新词、俚语、特定领域术语无能为力；\n若词典覆盖不全，系统容易失误；\n文本长度越短，语义越难判断准确。"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#summary-table",
    "href": "02_topic_sentiment/lecture2.html#summary-table",
    "title": "Lecture 2",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\nTopic\nKey Concept\nLimitation\n\n\n\n\nLanguage Modeling\nPredict next word\nSensitive to data sparsity\n\n\nPerplexity\nMeasures model’s confusion\nLower = better\n\n\nBoW\nSimple vector representation\nIgnores word order\n\n\nTF-IDF\nPenalizes common words\nStill context-free\n\n\nN-gram\nUses local context\nStruggles with OOV words\n\n\nLexicon Sentiment\nDictionary-based polarity\nPoor generalization"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#what-are-topic-models",
    "href": "02_topic_sentiment/lecture2.html#what-are-topic-models",
    "title": "Lecture 2",
    "section": "What Are Topic Models?",
    "text": "What Are Topic Models?\nTopic models are statistical methods that discover abstract “topics” in a collection of documents. The most well-known model is Latent Dirichlet Allocation (LDA). Documents are modeled as mixtures of topics, and each topic is characterized by a distribution over words.\n是一类无监督学习方法，能够从文本集合中自动发现潜在话题。\n\ndoc = 多个主题的混合；\ntopic = 一组词的概率分布。"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#lda-graphical-structure-plate-notation",
    "href": "02_topic_sentiment/lecture2.html#lda-graphical-structure-plate-notation",
    "title": "Lecture 2",
    "section": "LDA Graphical Structure & Plate Notation",
    "text": "LDA Graphical Structure & Plate Notation\n\n\nFigure: LDA model structure (adapted from Blei et al., 2003)\n\n\nα：hyperparameter for Dirichlet distribution of topic probabilities, 控制文档中主题分布的 Dirichlet 先验\nθ_d：topic probabilities for document d of the D documents, 第 d 篇文档的主题分布\nz_dn：第 d 篇文档中第 n 个词的主题\nβ_k：word probabilities given topic k, 第 k 个主题下的词分布\nw_dn：实际生成的词\nη：控制 β 的 Dirichlet 超参数\n\n生成过程：\n\n对每篇文档 d，从 Dirichlet(α) 抽样出 θ_d\n对每个词，从 Multinomial(θ_d) 选择主题 z_dn\n再从 Multinomial(β_{z_dn}) 中采样出词 w_dn"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#lda-as-soft-clustering",
    "href": "02_topic_sentiment/lecture2.html#lda-as-soft-clustering",
    "title": "Lecture 2",
    "section": "LDA as Soft Clustering",
    "text": "LDA as Soft Clustering\nLDA 是一种Soft Clustering\n\nEach word is probabilistically assigned to a topic.\nUnlike hard clustering (e.g. K-means), LDA allows multiple topics per document.\n常见的聚类模型，比如 K-means，是一种Hard Clustering,每个文档只属于一个类。\nLDA是Soft Clustering: 每个词都有一个属于不同主题的概率;\n每个词是从多个主题中以概率方式抽样得到的；\n每个文档可能属于多个主题；\n每个文档的主题分布是一个 Multinomial, 参数来自 Dirichlet"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#lda-as-dimension-reduction",
    "href": "02_topic_sentiment/lecture2.html#lda-as-dimension-reduction",
    "title": "Lecture 2",
    "section": "LDA as Dimension Reduction",
    "text": "LDA as Dimension Reduction\nLDA 也可以看作是一种降维技术： * 把高维的词频向量映射为低维的主题空间； * 非线性降维 \\[\n\\mathbb{E}[w] = \\theta^\\top \\beta\n\\]\n\nθ：文档的主题分布（类似 PCA 中的因子得分）\nβ：主题中的词分布（类似因子载荷）\n\n\n与 PCA 的异同：\n\n\n\n比较\nPCA\nLDA\n\n\n\n\n分量符号\n可正可负\n都是非负\n\n\n模型类型\n线性代数\n贝叶斯生成模型\n\n\n表达方式\n向量投影\n概率生成\n\n\n数据类型\n连续数据\n离散数据（如文本）\n\n\n是否考虑词共现\n否\n是（通过主题）\n\n\n\n\nLDA与 PCA 类似，但：\n\nLDA 的分量是正数\n用概率建模而非线性投影\n\n\n替代方案：Matrix Factorization、Alternating Least Squares 交替最小二乘（ALS）"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#variational-inference-in-lda",
    "href": "02_topic_sentiment/lecture2.html#variational-inference-in-lda",
    "title": "Lecture 2",
    "section": "Variational Inference in LDA",
    "text": "Variational Inference in LDA\nWhy Variational Inference (VI)?\n\nPosterior \\(p(\\theta, z \\mid w)\\) is intractable.\nVI approximates it with simpler \\(q(\\theta, z)\\) by minimizing KL divergence.\n\nVI Mechanism: * 贝叶斯推断要求计算后验分布，但没有解析解； \\[\np(\\theta \\mid \\text{data}) = \\frac{p(\\text{data} \\mid \\theta) p(\\theta)}{p(\\text{data})}\n\\]\n\nVI 把推断问题转为优化问题：\n\n选一个可处理的分布族 \\(q\\)，使其最接近真实后验。\n用变分参数 \\(\\gamma\\) 和 \\(\\phi\\) 表达文档的主题表示和词的主题分布。\n\nUse mean-field factorization: \\(q(\\theta, z) = q(\\theta) \\prod_n q(z_n)\\)\nOptimize parameters \\(\\gamma, \\phi\\)\nIteratively update:\n\n\\(\\phi_{ni} \\propto \\beta_{i w_n} \\exp(\\Psi(\\gamma_i))\\)\n\\(\\gamma_i = \\alpha_i + \\sum_n \\phi_{ni}\\) ### 两种主要的后验近似方法\n\n\n\n1. MCMC（Markov Chain Monte Carlo）\n\n样本采样模拟后验分布；\n理论上准确（收敛到真实后验），但：\n\n计算代价高；\n收敛慢；\n对于大规模文档数据不现实。\n\n\n\n\n2. Variational Inference（VI）\nVI 的基本思想是：\n与其反复抽样后验分布，不如选择一个容易处理的分布族 \\(q(\\theta, z)\\)，通过优化来逼近真实后验 \\(p(\\theta, z \\mid w)\\)。\n具体做法是：\n\n选择一个易于计算的变分分布族 \\(q(\\theta, z)\\)（如 Dirichlet × Multinomial）；\n通过最小化 KL 散度（Kullback-Leibler Divergence）：\n\n\\[\nq^*(\\theta, z) = \\arg\\min_q \\text{KL}(q(\\theta, z) \\| p(\\theta, z \\mid w))\n\\]\n这个最优化问题等价于最大化 Evidence Lower Bound。\n比较：\n\n\n\n方法\n优点\n缺点\n\n\n\n\nMCMC\n精度高\n慢，不适合大规模\n\n\nVI\n快，有解析形式\n有偏差，依赖分布族"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#extensions-of-lda",
    "href": "02_topic_sentiment/lecture2.html#extensions-of-lda",
    "title": "Lecture 2",
    "section": "Extensions of LDA",
    "text": "Extensions of LDA\n\n\n\n扩展模型\n描述\n\n\n\n\nCTM（Correlated Topic Model）\n使用 Logistic Normal 建模主题相关性\n\n\nAuthor-Topic Model\n每个作者拥有一个主题分布\n\n\nDynamic Topic Model\n模型中主题会随时间变化\n\n\n\n这些扩展往往破坏 Dirichlet 的简洁性质，使得推断变得更复杂（通常需要 MCMC 或 EP）"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#sentence-constrained-lda",
    "href": "02_topic_sentiment/lecture2.html#sentence-constrained-lda",
    "title": "Lecture 2",
    "section": "Sentence-Constrained LDA",
    "text": "Sentence-Constrained LDA\nSC-LDA 假设：一句话通常只讲一个主题，并且这个主题可能在下一句中延续。\n\n每句 → 一个主题\n更贴近用户在评论中的表达结构\n可用于产品属性提取（如“屏幕、价格、电池”）\n文档的主题分布可用于预测评分\nEach sentence tends to express one topic (Buschken & Allenby, 2016)\nBetter topic-word tail modeling\nImproves interpretability & rating prediction\n主题由“名词”驱动，情感由“形容词”驱动；\n同一个主题可能包含褒贬不一的内容；\n在大数据中可能学出“主题+情绪”复合结构（如“bad battery”）\n\nTopic × Sentiment:\n\n先识别句子的主题（SC-LDA）\n对句子做情绪分类\n将情绪与主题进行交互建模 → 方面级情感分析（Aspect-Based Sentiment Analysis）"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#summary",
    "href": "02_topic_sentiment/lecture2.html#summary",
    "title": "Lecture 2",
    "section": "Summary",
    "text": "Summary\n\n\n\n主题\n内容\n\n\n\n\nLDA 核心\n主题分布 × 词分布（θ × β）\n\n\n推断方法\nVI（快） vs MCMC（准）\n\n\n模型结构\n文档 = 多主题，词 = 从主题中抽样\n\n\n变体\nCTM, Dynamic LDA, Author-Topic, SC-LDA\n\n\n进阶方向\n加入情绪、时间、结构化先验"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#references",
    "href": "02_topic_sentiment/lecture2.html#references",
    "title": "Lecture 2",
    "section": "References",
    "text": "References\n\nHu, M., & Liu, B. (2004). Mining and summarizing customer reviews.\nAgarwal, A., & Mittal, N. (2015). Text classification using machine learning methods.\nWordNet: https://wordnet.princeton.edu/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nlp-notes",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]