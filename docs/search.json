[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#language-modeling-and-probability-foundations",
    "href": "02_topic_sentiment/lecture2.html#language-modeling-and-probability-foundations",
    "title": "Video Notes: Ngram, language model, sentiment analysis",
    "section": "Language Modeling and Probability Foundations",
    "text": "Language Modeling and Probability Foundations\n\nSequence Modeling and Prediction\nLanguage models predict the next word in a sequence given the preceding words. This is typically modeled using conditional probability:\n语言模型的基本任务是：根据前面的词来预测下一个词。通常使用**条件概率（Conditional Probability）**来表示：\n\\[\nP(w_5 \\mid w_1, w_2, w_3, w_4)\n\\]\nThis expresses the probability of the 5th word given the first four.\n\nExample:\nSentence: “Luck is what happens when preparation meets opportunity.”\n我们可以问：在“Luck is what happens”之后出现“when”的概率是多少？\n\n\n\nChain Rule of Probability\nTo compute complex conditional probabilities, we use the chain rule:\n为了计算复杂的条件概率，我们使用概率的链式法则（Chain Rule）：\n\nBasic rule: \\(P(A \\cap B) = P(A) P(B|A)\\)\nConditional probability definition: \\(P(B|A) = \\frac{P(A \\cap B)}{P(A)}\\)\nGeneralized form: \\(P(A \\cap B \\cap C) = P(C|B \\cap A) P(B|A) P(A)\\)\n\nIn the context of language:\n目标是估计：\n\\[\nP(\\text{when} \\mid \\text{happens}, \\text{what}, \\text{is}, \\text{luck})\n\\]\n使用链式法则展开：\n\\[\nP(\\text{when}) = P(\\text{when} \\mid \\text{happens} \\cap \\text{what} \\cap \\text{is} \\cap \\text{luck}) \\\\\n\\cdot P(\\text{happens} \\mid \\text{what} \\cap \\text{is} \\cap \\text{luck}) \\\\\n\\cdot P(\\text{what} \\mid \\text{is} \\cap \\text{luck}) \\\\\n\\cdot P(\\text{is} \\mid \\text{luck}) \\\\\n\\cdot P(\\text{luck})\n\\]\n这就是语言模型中常用的思路：将一个句子中各个词的联合概率转化为一系列条件概率的乘积。"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#perplexity-and-cross-entropy",
    "href": "02_topic_sentiment/lecture2.html#perplexity-and-cross-entropy",
    "title": "Video Notes: Ngram, language model, sentiment analysis",
    "section": "Perplexity and Cross-Entropy",
    "text": "Perplexity and Cross-Entropy\n\nPerplexity\nPerplexity evaluates how well a probability model predicts a sample:\n\nPerplexity = 2 raised to the cross-entropy of the model\n\nPerplexity（困惑度）衡量的是模型对测试集的“困惑”程度，越小越好。\n它等价于模型每预测一个词时的不确定度。\n\\[\n\\text{Perplexity} = 2^{-\\frac{1}{W} \\sum_{k=1}^{n} \\log P(w_k)}\n\\]\n其中：\n\n\\(W\\)：测试集中单词总数\n\\(P(w_k)\\)：模型给第 \\(k\\) 个词的概率预测\n\n\n\nCross-Entropy\nCross-entropy quantifies the average number of bits needed to encode data from a distribution \\(\\tilde{p}\\) using a model \\(q\\):\n交叉熵（Cross-Entropy）用来衡量一个分布 \\(\\tilde{p}\\) 和模型分布 \\(q\\) 之间的距离：\n\\[\nH(\\tilde{p}, q) = -\\sum_{i=1}^N \\tilde{p}(x_i) \\log_2 q(x_i)\n\\]\n其中：\n\n\\(\\tilde{p}(x_i)\\)：经验概率（词 \\(x_i\\) 的出现频率）\n\\(q(x_i)\\)：模型预测的概率\n\n和 Perplexity 的关系：\n\\[\n\\text{Perplexity} = 2^{\\text{Cross-Entropy}}\n\\]\n\n\nKL Divergence (Relative Entropy)\nKL 散度衡量一个近似分布 \\(q\\) 与真实分布 \\(p\\) 的差异。它不对称，但如果两者相等，KL 散度为 0。\n公式：\n\\[\nD_{KL}(p \\parallel q) = \\sum_{i=1}^N p(x_i) \\log \\left( \\frac{p(x_i)}{q(x_i)} \\right)\n\\]\n\n\\(p(x_i)\\)：真实分布\n\\(q(x_i)\\)：模型的近似分布\n\n用途：KL 散度在训练过程中常用于优化目标函数，比如通过最小化 KL 散度来逼近真实分布。"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#text-representation-models",
    "href": "02_topic_sentiment/lecture2.html#text-representation-models",
    "title": "Video Notes: Ngram, language model, sentiment analysis",
    "section": "Text Representation Models",
    "text": "Text Representation Models\n\nBag-of-Words (BoW)\nConverts text into vectors based on word occurrence counts.\nBoW（词袋模型）将文本向量化，每个维度表示词表中一个词在句子中出现的频率。\n\n例子说明：\n句子：\nI WOULD NOT, COULD NOT IN THE RAIN.\nNOT IN THE DARK. NOT ON A TRAIN.\n被转换为两个向量：\n[1, 1, 1, 2, 1, 1, 1, 1, 0, 0, ..., 0]\n[0, 0, 0, 2, 1, 1, 0, 1, 1, 1, ..., 0]\n每个数表示词表中对应单词在该“文档”（即句子）中出现的次数。\n\n\n\nLimitations of BoW\nBoW 的主要缺点：\n\n不考虑语义（Semantic meaning）：忽略上下文，比如“我爱你”和“你爱我”会被视为相同向量。\n维度高（Vector size）：词表大时，向量稀疏、占用资源多。\n\n\n\nTF-IDF (Term Frequency-Inverse Document Frequency)\nGives less importance to common words:\n目的：降低高频无意义词（如 “the”, “and”）的权重，提高有区分度的词的重要性。\n定义：\n\nTF（Term Frequency）：词在文档中的频率；\nIDF（Inverse Document Frequency）：词在整个文档集中出现的“逆频率”。\n\n公式：\n\\[\n\\text{TF-IDF}(t, d) = tf(t, d) \\cdot \\log \\left( \\frac{N}{df(t)} \\right)\n\\]\n其中：\n\n\\(t\\)：词\n\\(d\\)：文档\n\\(df(t)\\)：包含该词的文档数\n\\(N\\)：文档总数\n\n\n\nNormalized Term Frequency\n为了解决文档长短对词频带来的偏差，可以进行规范化处理：\n\n对数缩放（Log Normalization）： \\(\\log(1 + f_{t,d})\\)\n最大值归一化（Max Normalization）：\n\\[\ntf(t,d) = 0.5 + 0.5 \\cdot \\frac{f_{t,d}}{\\max \\{f_{t',d} : t' \\in d\\}}\n\\]"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#n-gram-models",
    "href": "02_topic_sentiment/lecture2.html#n-gram-models",
    "title": "Video Notes: Ngram, language model, sentiment analysis",
    "section": "N-gram Models",
    "text": "N-gram Models\n\nBi-grams and N-grams\nBi-gram（双词模型）解决 BoW 忽略词序的问题，将每两个词作为一个单元进行建模。\n例如：\n\"Luck is\", \"is what\", \"what happens\", ...\n\n\nN-gram 一般化\nN-gram 是对 Bi-gram 的拓展，使用前 \\(N-1\\) 个词预测当前词。\n\n数学表达式：\n\nUnigram: \\(P(w_i) = \\frac{\\text{Count}(w_i)}{\\sum_j \\text{Count}(w_j)}\\)\nBigram: \\(P(w_i | w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}\\)\nN-gram: \\(P(w_1^n) = \\prod_{k=1}^{n} P(w_k | w_{k-1})\\)\n\n表示整句话的概率是各个词的条件概率连乘。\n\n\n\nOut-of-Vocabulary Words（词表外词）\n当训练集中没有出现的词叫 OOV（Out-of-Vocabulary），这会导致概率为零。\n解决办法：\n\n扩大语料库（Increase corpus size）\n跳过缺失的 n-gram（Leap over missing n-grams）\n使用平滑技术（Smoothing），如： \\(P_{\\text{Laplace}}(w_i) = \\frac{c_i + 1}{N + V}\\) 其中：\n\n\\(c_i\\)：词 \\(w_i\\) 出现次数\n\\(N\\)：词总数\n\\(V\\)：词表大小"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#sentiment-analysis-and-semantic-orientation",
    "href": "02_topic_sentiment/lecture2.html#sentiment-analysis-and-semantic-orientation",
    "title": "Video Notes: Ngram, language model, sentiment analysis",
    "section": "Sentiment Analysis and Semantic Orientation",
    "text": "Sentiment Analysis and Semantic Orientation\n\nDefinition\nSemantic orientation analyzes sentiment strength and direction in text.\n语义倾向（Semantic Orientation）用于判断文本中单词或短语的情绪极性和强度。\n\nKey dimensions:\n\nSubjectivity: objective vs subjective （客观 vs 主观）\nPolarity: negative vs positive （负面 vs 正面）\nIntensity: strength (e.g. “very good” vs “good”)\n\n\n\n\nLexicon-Based Approach\n基于人工或半自动构建的情感词典，如 Hu & Liu（2004）的方法：\n\n提取 意见词（opinion words），如形容词（good, bad, amazing）；\n使用 WordNet 获取同义词/反义词，推断其情绪倾向；\n逐词分析并合并为句子级别判断。\n\n\n\nWordNet Role\nWordNet 是一个词汇知识库，它用语义关系连接单词，包括：\n\nSynonym（同义词）：形成同义集 Synsets\nAntonym（反义词）：如 wet 与 dry\n上下位关系（Hypernym-Hyponym）：如 furniture &gt; bed\n部分整体关系（Meronym）：如 chair 与 backrest\n\n\n\nLimitations of Lexicon Methods\n引用自 Agarwal & Mittal（2015）：\n主要问题：\n\n依赖训练语料库中的已有词汇；\n对新词、俚语、特定领域术语无能为力；\n若词典覆盖不全，系统容易失误；\n文本长度越短，语义越难判断准确。"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#summary-table",
    "href": "02_topic_sentiment/lecture2.html#summary-table",
    "title": "Video Notes: Ngram, language model, sentiment analysis",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\nTopic\nKey Concept\nLimitation\n\n\n\n\nLanguage Modeling\nPredict next word\nSensitive to data sparsity\n\n\nPerplexity\nMeasures model’s confusion\nLower = better\n\n\nBoW\nSimple vector representation\nIgnores word order\n\n\nTF-IDF\nPenalizes common words\nStill context-free\n\n\nN-gram\nUses local context\nStruggles with OOV words\n\n\nLexicon Sentiment\nDictionary-based polarity\nPoor generalization"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#references",
    "href": "02_topic_sentiment/lecture2.html#references",
    "title": "Video Notes: Ngram, language model, sentiment analysis",
    "section": "References",
    "text": "References\n\nHu, M., & Liu, B. (2004). Mining and summarizing customer reviews.\nAgarwal, A., & Mittal, N. (2015). Text classification using machine learning methods.\nWordNet: https://wordnet.princeton.edu/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nlp-notes",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]