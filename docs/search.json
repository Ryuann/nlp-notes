[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#language-modeling-and-probability-foundations",
    "href": "02_topic_sentiment/lecture2.html#language-modeling-and-probability-foundations",
    "title": "Lecture 2",
    "section": "Language Modeling and Probability Foundations",
    "text": "Language Modeling and Probability Foundations\n\nSequence Modeling and Prediction\nLanguage models predict the next word in a sequence given the preceding words. This is typically modeled using conditional probability:\nè¯­è¨€æ¨¡å‹çš„åŸºæœ¬ä»»åŠ¡æ˜¯ï¼šæ ¹æ®å‰é¢çš„è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚ç”¨Conditional Probabilityè¡¨ç¤ºï¼š\n\\[\nP(w_5 \\mid w_1, w_2, w_3, w_4)\n\\]\nThis expresses the probability of the 5th word given the first four.\n\nExample:\nSentence: â€œLuck is what happens when preparation meets opportunity.â€\nåœ¨â€œLuck is what happensâ€ä¹‹åå‡ºç°â€œwhenâ€çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ\n\n\n\nChain Rule of Probability\nTo compute complex conditional probabilities, we use the chain rule:\nä¸ºäº†è®¡ç®—å¤æ‚çš„æ¡ä»¶æ¦‚ç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¦‚ç‡çš„Chain Ruleï¼š\n\nBasic rule: \\(P(A \\cap B) = P(A) P(B|A)\\)\nConditional probability definition: \\(P(B|A) = \\frac{P(A \\cap B)}{P(A)}\\)\nGeneralized form: \\(P(A \\cap B \\cap C) = P(C|B \\cap A) P(B|A) P(A)\\)\n\nIn the context of language:\nç›®æ ‡æ˜¯ä¼°è®¡ï¼š\n\\[\nP(\\text{when} \\mid \\text{happens}, \\text{what}, \\text{is}, \\text{luck})\n\\]\nä½¿ç”¨chain ruleå±•å¼€ï¼š\n\\[\nP(\\text{when}) = P(\\text{when} \\mid \\text{happens} \\cap \\text{what} \\cap \\text{is} \\cap \\text{luck}) \\\\\n\\cdot P(\\text{happens} \\mid \\text{what} \\cap \\text{is} \\cap \\text{luck}) \\\\\n\\cdot P(\\text{what} \\mid \\text{is} \\cap \\text{luck}) \\\\\n\\cdot P(\\text{is} \\mid \\text{luck}) \\\\\n\\cdot P(\\text{luck})\n\\]\nè¿™å°±æ˜¯è¯­è¨€æ¨¡å‹ä¸­å¸¸ç”¨çš„æ€è·¯ï¼šå°†ä¸€ä¸ªå¥å­ä¸­å„ä¸ªè¯çš„è”åˆæ¦‚ç‡è½¬åŒ–ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯ã€‚"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#perplexity-and-cross-entropy",
    "href": "02_topic_sentiment/lecture2.html#perplexity-and-cross-entropy",
    "title": "Lecture 2",
    "section": "Perplexity and Cross-Entropy",
    "text": "Perplexity and Cross-Entropy\n\nPerplexity\nPerplexity evaluates how well a probability model predicts a sample:\n\nPerplexity = 2 raised to the cross-entropy of the model\n\nPerplexityè¡¡é‡çš„æ˜¯æ¨¡å‹å¯¹æµ‹è¯•é›†çš„â€œå›°æƒ‘â€ç¨‹åº¦ï¼Œè¶Šå°è¶Šå¥½ã€‚\nå®ƒç­‰ä»·äºæ¨¡å‹æ¯é¢„æµ‹ä¸€ä¸ªè¯æ—¶çš„ä¸ç¡®å®šåº¦ã€‚\n\\[\n\\text{Perplexity} = 2^{-\\frac{1}{W} \\sum_{k=1}^{n} \\log P(w_k)}\n\\]\nå…¶ä¸­ï¼š\n\n\\(W\\)ï¼šæµ‹è¯•é›†ä¸­å•è¯æ€»æ•°\n\\(P(w_k)\\)ï¼šæ¨¡å‹ç»™ç¬¬ \\(k\\) ä¸ªè¯çš„æ¦‚ç‡é¢„æµ‹\n\n\n\nCross-Entropy\nCross-entropy quantifies the average number of bits needed to encode data from a distribution \\(\\tilde{p}\\) using a model \\(q\\):\näº¤å‰ç†µï¼ˆCross-Entropyï¼‰ç”¨æ¥è¡¡é‡ä¸€ä¸ªåˆ†å¸ƒ \\(\\tilde{p}\\) å’Œæ¨¡å‹åˆ†å¸ƒ \\(q\\) ä¹‹é—´çš„è·ç¦»ï¼š\n\\[\nH(\\tilde{p}, q) = -\\sum_{i=1}^N \\tilde{p}(x_i) \\log_2 q(x_i)\n\\]\nå…¶ä¸­ï¼š\n\n\\(\\tilde{p}(x_i)\\)ï¼šç»éªŒæ¦‚ç‡ï¼ˆè¯ \\(x_i\\) çš„å‡ºç°é¢‘ç‡ï¼‰\n\\(q(x_i)\\)ï¼šæ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡\n\nå’Œ Perplexity çš„å…³ç³»ï¼š\n\\[\n\\text{Perplexity} = 2^{\\text{Cross-Entropy}}\n\\]\n\n\nKL Divergence (Relative Entropy)\nKL æ•£åº¦è¡¡é‡ä¸€ä¸ªè¿‘ä¼¼åˆ†å¸ƒ \\(q\\) ä¸çœŸå®åˆ†å¸ƒ \\(p\\) çš„å·®å¼‚ã€‚å®ƒä¸å¯¹ç§°ï¼Œä½†å¦‚æœä¸¤è€…ç›¸ç­‰ï¼ŒKL æ•£åº¦ä¸º 0ã€‚\nå…¬å¼ï¼š\n\\[\nD_{KL}(p \\parallel q) = \\sum_{i=1}^N p(x_i) \\log \\left( \\frac{p(x_i)}{q(x_i)} \\right)\n\\]\n\n\\(p(x_i)\\)ï¼šçœŸå®åˆ†å¸ƒ\n\\(q(x_i)\\)ï¼šæ¨¡å‹çš„è¿‘ä¼¼åˆ†å¸ƒ\n\nç”¨é€”ï¼šKL æ•£åº¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸ç”¨äºä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œæ¯”å¦‚é€šè¿‡æœ€å°åŒ– KL æ•£åº¦æ¥é€¼è¿‘çœŸå®åˆ†å¸ƒã€‚"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#text-representation-models",
    "href": "02_topic_sentiment/lecture2.html#text-representation-models",
    "title": "Lecture 2",
    "section": "Text Representation Models",
    "text": "Text Representation Models\n\nBag-of-Words (BoW)\nConverts text into vectors based on word occurrence counts.\nBoWï¼ˆè¯è¢‹æ¨¡å‹ï¼‰å°†æ–‡æœ¬å‘é‡åŒ–ï¼Œæ¯ä¸ªç»´åº¦è¡¨ç¤ºè¯è¡¨ä¸­ä¸€ä¸ªè¯åœ¨å¥å­ä¸­å‡ºç°çš„é¢‘ç‡ã€‚\n\nä¾‹å­è¯´æ˜ï¼š\nå¥å­ï¼š\nI WOULD NOT, COULD NOT IN THE RAIN.\nNOT IN THE DARK. NOT ON A TRAIN.\nè¢«è½¬æ¢ä¸ºä¸¤ä¸ªå‘é‡ï¼š\n[1, 1, 1, 2, 1, 1, 1, 1, 0, 0, ..., 0]\n[0, 0, 0, 2, 1, 1, 0, 1, 1, 1, ..., 0]\næ¯ä¸ªæ•°è¡¨ç¤ºè¯è¡¨ä¸­å¯¹åº”å•è¯åœ¨è¯¥â€œæ–‡æ¡£â€ï¼ˆå³å¥å­ï¼‰ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚\n\n\n\nLimitations of BoW\nBoW çš„ä¸»è¦ç¼ºç‚¹ï¼š\n\nä¸è€ƒè™‘è¯­ä¹‰ï¼ˆSemantic meaningï¼‰ï¼šå¿½ç•¥ä¸Šä¸‹æ–‡ï¼Œæ¯”å¦‚â€œæˆ‘çˆ±ä½ â€å’Œâ€œä½ çˆ±æˆ‘â€ä¼šè¢«è§†ä¸ºç›¸åŒå‘é‡ã€‚\nç»´åº¦é«˜ï¼ˆVector sizeï¼‰ï¼šè¯è¡¨å¤§æ—¶ï¼Œå‘é‡ç¨€ç–ã€å ç”¨èµ„æºå¤šã€‚\n\n\n\nTF-IDF (Term Frequency-Inverse Document Frequency)\nGives less importance to common words:\nç›®çš„ï¼šé™ä½é«˜é¢‘æ— æ„ä¹‰è¯ï¼ˆå¦‚ â€œtheâ€, â€œandâ€ï¼‰çš„æƒé‡ï¼Œæé«˜æœ‰åŒºåˆ†åº¦çš„è¯çš„é‡è¦æ€§ã€‚\nå®šä¹‰ï¼š\n\nTFï¼ˆTerm Frequencyï¼‰ï¼šè¯åœ¨æ–‡æ¡£ä¸­çš„é¢‘ç‡ï¼›\nIDFï¼ˆInverse Document Frequencyï¼‰ï¼šè¯åœ¨æ•´ä¸ªæ–‡æ¡£é›†ä¸­å‡ºç°çš„â€œé€†é¢‘ç‡â€ã€‚\n\nå…¬å¼ï¼š\n\\[\n\\text{TF-IDF}(t, d) = tf(t, d) \\cdot \\log \\left( \\frac{N}{df(t)} \\right)\n\\]\nå…¶ä¸­ï¼š\n\n\\(t\\)ï¼šè¯\n\\(d\\)ï¼šæ–‡æ¡£\n\\(df(t)\\)ï¼šåŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°\n\\(N\\)ï¼šæ–‡æ¡£æ€»æ•°\n\n\n\nNormalized Term Frequency\nä¸ºäº†è§£å†³æ–‡æ¡£é•¿çŸ­å¯¹è¯é¢‘å¸¦æ¥çš„åå·®ï¼Œå¯ä»¥è¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼š\n\nå¯¹æ•°ç¼©æ”¾ï¼ˆLog Normalizationï¼‰ï¼š \\(\\log(1 + f_{t,d})\\)\næœ€å¤§å€¼å½’ä¸€åŒ–ï¼ˆMax Normalizationï¼‰ï¼š\n\\[\ntf(t,d) = 0.5 + 0.5 \\cdot \\frac{f_{t,d}}{\\max \\{f_{t',d} : t' \\in d\\}}\n\\]"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#n-gram-models",
    "href": "02_topic_sentiment/lecture2.html#n-gram-models",
    "title": "Lecture 2",
    "section": "N-gram Models",
    "text": "N-gram Models\n\nBi-grams and N-grams\nBi-gramï¼ˆåŒè¯æ¨¡å‹ï¼‰è§£å†³ BoW å¿½ç•¥è¯åºçš„é—®é¢˜ï¼Œå°†æ¯ä¸¤ä¸ªè¯ä½œä¸ºä¸€ä¸ªå•å…ƒè¿›è¡Œå»ºæ¨¡ã€‚\nä¾‹å¦‚ï¼š\n\"Luck is\", \"is what\", \"what happens\", ...\n\n\nN-gram ä¸€èˆ¬åŒ–\nN-gram æ˜¯å¯¹ Bi-gram çš„æ‹“å±•ï¼Œä½¿ç”¨å‰ \\(N-1\\) ä¸ªè¯é¢„æµ‹å½“å‰è¯ã€‚\n\næ•°å­¦è¡¨è¾¾å¼ï¼š\n\nUnigram: \\(P(w_i) = \\frac{\\text{Count}(w_i)}{\\sum_j \\text{Count}(w_j)}\\)\nBigram: \\(P(w_i | w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}\\)\nN-gram: \\(P(w_1^n) = \\prod_{k=1}^{n} P(w_k | w_{k-1})\\)\n\nè¡¨ç¤ºæ•´å¥è¯çš„æ¦‚ç‡æ˜¯å„ä¸ªè¯çš„æ¡ä»¶æ¦‚ç‡è¿ä¹˜ã€‚\n\n\n\nOut-of-Vocabulary Wordsï¼ˆè¯è¡¨å¤–è¯ï¼‰\nå½“è®­ç»ƒé›†ä¸­æ²¡æœ‰å‡ºç°çš„è¯å« OOVï¼ˆOut-of-Vocabularyï¼‰ï¼Œè¿™ä¼šå¯¼è‡´æ¦‚ç‡ä¸ºé›¶ã€‚\nè§£å†³åŠæ³•ï¼š\n\næ‰©å¤§è¯­æ–™åº“ï¼ˆIncrease corpus sizeï¼‰\nè·³è¿‡ç¼ºå¤±çš„ n-gramï¼ˆLeap over missing n-gramsï¼‰\nä½¿ç”¨å¹³æ»‘æŠ€æœ¯ï¼ˆSmoothingï¼‰ï¼Œå¦‚ï¼š \\(P_{\\text{Laplace}}(w_i) = \\frac{c_i + 1}{N + V}\\) å…¶ä¸­ï¼š\n\n\\(c_i\\)ï¼šè¯ \\(w_i\\) å‡ºç°æ¬¡æ•°\n\\(N\\)ï¼šè¯æ€»æ•°\n\\(V\\)ï¼šè¯è¡¨å¤§å°"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#sentiment-analysis-and-semantic-orientation",
    "href": "02_topic_sentiment/lecture2.html#sentiment-analysis-and-semantic-orientation",
    "title": "Lecture 2",
    "section": "Sentiment Analysis and Semantic Orientation",
    "text": "Sentiment Analysis and Semantic Orientation\n\nDefinition\nSemantic orientation analyzes sentiment strength and direction in text.\nè¯­ä¹‰å€¾å‘ï¼ˆSemantic Orientationï¼‰ç”¨äºåˆ¤æ–­æ–‡æœ¬ä¸­å•è¯æˆ–çŸ­è¯­çš„æƒ…ç»ªææ€§å’Œå¼ºåº¦ã€‚\n\nKey dimensions:\n\nSubjectivity: objective vs subjective ï¼ˆå®¢è§‚ vs ä¸»è§‚ï¼‰\nPolarity: negative vs positive ï¼ˆè´Ÿé¢ vs æ­£é¢ï¼‰\nIntensity: strength (e.g.Â â€œvery goodâ€ vs â€œgoodâ€)\n\n\n\n\nLexicon-Based Approach\nåŸºäºäººå·¥æˆ–åŠè‡ªåŠ¨æ„å»ºçš„æƒ…æ„Ÿè¯å…¸ï¼Œå¦‚ Hu & Liuï¼ˆ2004ï¼‰çš„æ–¹æ³•ï¼š\n\næå– æ„è§è¯ï¼ˆopinion wordsï¼‰ï¼Œå¦‚å½¢å®¹è¯ï¼ˆgood, bad, amazingï¼‰ï¼›\nä½¿ç”¨ WordNet è·å–åŒä¹‰è¯/åä¹‰è¯ï¼Œæ¨æ–­å…¶æƒ…ç»ªå€¾å‘ï¼›\né€è¯åˆ†æå¹¶åˆå¹¶ä¸ºå¥å­çº§åˆ«åˆ¤æ–­ã€‚\n\n\n\nWordNet Role\nWordNet æ˜¯ä¸€ä¸ªè¯æ±‡çŸ¥è¯†åº“ï¼Œå®ƒç”¨è¯­ä¹‰å…³ç³»è¿æ¥å•è¯ï¼ŒåŒ…æ‹¬ï¼š\n\nSynonymï¼ˆåŒä¹‰è¯ï¼‰ï¼šå½¢æˆåŒä¹‰é›† Synsets\nAntonymï¼ˆåä¹‰è¯ï¼‰ï¼šå¦‚ wet ä¸ dry\nä¸Šä¸‹ä½å…³ç³»ï¼ˆHypernym-Hyponymï¼‰ï¼šå¦‚ furniture &gt; bed\néƒ¨åˆ†æ•´ä½“å…³ç³»ï¼ˆMeronymï¼‰ï¼šå¦‚ chair ä¸ backrest\n\n\n\nLimitations of Lexicon Methods\nå¼•ç”¨è‡ª Agarwal & Mittalï¼ˆ2015ï¼‰ï¼š\nä¸»è¦é—®é¢˜ï¼š\n\nä¾èµ–è®­ç»ƒè¯­æ–™åº“ä¸­çš„å·²æœ‰è¯æ±‡ï¼›\nå¯¹æ–°è¯ã€ä¿šè¯­ã€ç‰¹å®šé¢†åŸŸæœ¯è¯­æ— èƒ½ä¸ºåŠ›ï¼›\nè‹¥è¯å…¸è¦†ç›–ä¸å…¨ï¼Œç³»ç»Ÿå®¹æ˜“å¤±è¯¯ï¼›\næ–‡æœ¬é•¿åº¦è¶ŠçŸ­ï¼Œè¯­ä¹‰è¶Šéš¾åˆ¤æ–­å‡†ç¡®ã€‚"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#summary-table",
    "href": "02_topic_sentiment/lecture2.html#summary-table",
    "title": "Lecture 2",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\nTopic\nKey Concept\nLimitation\n\n\n\n\nLanguage Modeling\nPredict next word\nSensitive to data sparsity\n\n\nPerplexity\nMeasures modelâ€™s confusion\nLower = better\n\n\nBoW\nSimple vector representation\nIgnores word order\n\n\nTF-IDF\nPenalizes common words\nStill context-free\n\n\nN-gram\nUses local context\nStruggles with OOV words\n\n\nLexicon Sentiment\nDictionary-based polarity\nPoor generalization"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#what-are-topic-models",
    "href": "02_topic_sentiment/lecture2.html#what-are-topic-models",
    "title": "Lecture 2",
    "section": "What Are Topic Models?",
    "text": "What Are Topic Models?\nTopic models are statistical methods that discover abstract â€œtopicsâ€ in a collection of documents. The most well-known model is Latent Dirichlet Allocation (LDA). Documents are modeled as mixtures of topics, and each topic is characterized by a distribution over words.\næ˜¯ä¸€ç±»æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬é›†åˆä¸­è‡ªåŠ¨å‘ç°æ½œåœ¨è¯é¢˜ã€‚\n\ndoc = å¤šä¸ªä¸»é¢˜çš„æ··åˆï¼›\ntopic = ä¸€ç»„è¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#lda-graphical-structure-plate-notation",
    "href": "02_topic_sentiment/lecture2.html#lda-graphical-structure-plate-notation",
    "title": "Lecture 2",
    "section": "LDA Graphical Structure & Plate Notation",
    "text": "LDA Graphical Structure & Plate Notation\n\nÎ±ï¼šæ§åˆ¶æ–‡æ¡£ä¸­ä¸»é¢˜åˆ†å¸ƒçš„ Dirichlet å…ˆéªŒ\nÎ¸_dï¼šç¬¬ d ç¯‡æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒ\nz_dnï¼šç¬¬ d ç¯‡æ–‡æ¡£ä¸­ç¬¬ n ä¸ªè¯çš„ä¸»é¢˜\nÎ²_kï¼šç¬¬ k ä¸ªä¸»é¢˜ä¸‹çš„è¯åˆ†å¸ƒ\nw_dnï¼šå®é™…ç”Ÿæˆçš„è¯\nÎ·ï¼šæ§åˆ¶ Î² çš„ Dirichlet è¶…å‚æ•°\n\nğŸ§¾ ç”Ÿæˆè¿‡ç¨‹ï¼š\n\nå¯¹æ¯ç¯‡æ–‡æ¡£ dï¼Œä» Dirichlet(Î±) æŠ½æ ·å‡º Î¸_d\nå¯¹æ¯ä¸ªè¯ï¼Œä» Multinomial(Î¸_d) é€‰æ‹©ä¸»é¢˜ z_dn\nå†ä» Multinomial(Î²_{z_dn}) ä¸­é‡‡æ ·å‡ºè¯ w_dn"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#lda-as-soft-clustering",
    "href": "02_topic_sentiment/lecture2.html#lda-as-soft-clustering",
    "title": "Lecture 2",
    "section": "LDA as Soft Clustering",
    "text": "LDA as Soft Clustering\nLDA æ˜¯ä¸€ç§Soft Clusteringï¼‰ * Each word is probabilistically assigned to a topic. * Unlike hard clustering (e.g.Â K-means), LDA allows multiple topics per document.\n\næ¯ä¸ªè¯æ˜¯ä»å¤šä¸ªä¸»é¢˜ä¸­ä»¥æ¦‚ç‡æ–¹å¼æŠ½æ ·å¾—åˆ°çš„ï¼›\næ¯ä¸ªæ–‡æ¡£å¯èƒ½å±äºå¤šä¸ªä¸»é¢˜ï¼›\nåŒºåˆ«äº Latent Class æ¨¡å‹ï¼ˆç¡¬èšç±»ï¼Œæ¯ä¸ªæ–‡æ¡£åªå±äºä¸€ä¸ªä¸»é¢˜ï¼‰ã€‚"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#lda-as-dimension-reduction",
    "href": "02_topic_sentiment/lecture2.html#lda-as-dimension-reduction",
    "title": "Lecture 2",
    "section": "LDA as Dimension Reduction",
    "text": "LDA as Dimension Reduction\nLDA ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§é™ç»´æŠ€æœ¯ï¼š * æŠŠé«˜ç»´çš„è¯é¢‘å‘é‡æ˜ å°„ä¸ºä½ç»´çš„ä¸»é¢˜ç©ºé—´ï¼› * ä¸ PCA ç±»ä¼¼ï¼Œä½†ï¼š * LDA çš„åˆ†é‡æ˜¯æ­£æ•° * ç”¨æ¦‚ç‡å»ºæ¨¡è€Œéçº¿æ€§æŠ•å½±\nğŸ“Œ æ›¿ä»£æ–¹æ¡ˆï¼šçŸ©é˜µåˆ†è§£ï¼ˆMatrix Factorizationï¼‰ã€äº¤æ›¿æœ€å°äºŒä¹˜ï¼ˆALSï¼‰ * Expected word vector: \\(\\mathbb{E}[w] = \\theta^\\top \\beta\\) * Î¸: documentâ€™s topic distribution * Î²: topicâ€™s word distribution"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#variational-inference-in-lda",
    "href": "02_topic_sentiment/lecture2.html#variational-inference-in-lda",
    "title": "Lecture 2",
    "section": "Variational Inference in LDA",
    "text": "Variational Inference in LDA\nWhy Variational Inference (VI)?\n\nPosterior \\(p(\\theta, z \\mid w)\\) is intractable.\nVI approximates it with simpler \\(q(\\theta, z)\\) by minimizing KL divergence.\n\nVI Mechanism:\n\nUse mean-field factorization: \\(q(\\theta, z) = q(\\theta) \\prod_n q(z_n)\\)\nOptimize parameters \\(\\gamma, \\phi\\)\nIteratively update:\n\n\\(\\phi_{ni} \\propto \\beta_{i w_n} \\exp(\\Psi(\\gamma_i))\\)\n\\(\\gamma_i = \\alpha_i + \\sum_n \\phi_{ni}\\)\n\nè´å¶æ–¯æ¨æ–­è¦æ±‚è®¡ç®—åéªŒåˆ†å¸ƒï¼Œä½†ä¸å¯è§£æï¼›\nVI æŠŠæ¨æ–­é—®é¢˜è½¬ä¸ºä¼˜åŒ–é—®é¢˜ï¼š\n\né€‰ä¸€ä¸ªå¯å¤„ç†çš„åˆ†å¸ƒæ— \\(q\\)ï¼Œä½¿å…¶æœ€æ¥è¿‘çœŸå®åéªŒã€‚\nç”¨å˜åˆ†å‚æ•° \\(\\gamma\\) å’Œ \\(\\phi\\) è¡¨è¾¾æ–‡æ¡£çš„ä¸»é¢˜è¡¨ç¤ºå’Œè¯çš„ä¸»é¢˜åˆ†å¸ƒã€‚\n\n\næ¯”è¾ƒï¼š\n\n\n\næ–¹æ³•\nä¼˜ç‚¹\nç¼ºç‚¹\n\n\n\n\nMCMC\nç²¾åº¦é«˜\næ…¢ï¼Œä¸é€‚åˆå¤§è§„æ¨¡\n\n\nVI\nå¿«ï¼Œæœ‰è§£æå½¢å¼\næœ‰åå·®ï¼Œä¾èµ–åˆ†å¸ƒæ—"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#extensions-of-lda",
    "href": "02_topic_sentiment/lecture2.html#extensions-of-lda",
    "title": "Lecture 2",
    "section": "Extensions of LDA",
    "text": "Extensions of LDA\n\n\n\næ‰©å±•æ¨¡å‹\næè¿°\n\n\n\n\nCTMï¼ˆCorrelated Topic Modelï¼‰\nä½¿ç”¨ Logistic Normal å»ºæ¨¡ä¸»é¢˜ç›¸å…³æ€§\n\n\nAuthor-Topic Model\næ¯ä¸ªä½œè€…æ‹¥æœ‰ä¸€ä¸ªä¸»é¢˜åˆ†å¸ƒ\n\n\nDynamic Topic Model\næ¨¡å‹ä¸­ä¸»é¢˜ä¼šéšæ—¶é—´å˜åŒ–\n\n\n\nğŸ“Œ ç¼ºç‚¹ï¼šè¿™äº›æ‰©å±•å¾€å¾€ç ´å Dirichlet çš„ç®€æ´æ€§è´¨ï¼Œä½¿å¾—æ¨æ–­å˜å¾—æ›´å¤æ‚ï¼ˆé€šå¸¸éœ€è¦ MCMC æˆ– EPï¼‰"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#sentence-constrained-lda",
    "href": "02_topic_sentiment/lecture2.html#sentence-constrained-lda",
    "title": "Lecture 2",
    "section": "Sentence-Constrained LDA",
    "text": "Sentence-Constrained LDA\nSC-LDA å‡è®¾ï¼šä¸€å¥è¯é€šå¸¸åªè®²ä¸€ä¸ªä¸»é¢˜ï¼Œå¹¶ä¸”è¿™ä¸ªä¸»é¢˜å¯èƒ½åœ¨ä¸‹ä¸€å¥ä¸­å»¶ç»­ã€‚\n\næ¯å¥ â†’ ä¸€ä¸ªä¸»é¢˜\næ›´è´´è¿‘ç”¨æˆ·åœ¨è¯„è®ºä¸­çš„è¡¨è¾¾ç»“æ„\nå¯ç”¨äºäº§å“å±æ€§æå–ï¼ˆå¦‚â€œå±å¹•ã€ä»·æ ¼ã€ç”µæ± â€ï¼‰\næ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒå¯ç”¨äºé¢„æµ‹è¯„åˆ†\nEach sentence tends to express one topic (Buschken & Allenby, 2016)\nBetter topic-word tail modeling\nImproves interpretability & rating prediction\n\n\næƒ…æ„Ÿä¸æ˜¯ä¸»é¢˜ï¼š\n\nä¸»é¢˜ç”±â€œåè¯â€é©±åŠ¨ï¼Œæƒ…æ„Ÿç”±â€œå½¢å®¹è¯â€é©±åŠ¨ï¼›\nåŒä¸€ä¸ªä¸»é¢˜å¯èƒ½åŒ…å«è¤’è´¬ä¸ä¸€çš„å†…å®¹ï¼›\nåœ¨å¤§æ•°æ®ä¸­å¯èƒ½å­¦å‡ºâ€œä¸»é¢˜+æƒ…ç»ªâ€å¤åˆç»“æ„ï¼ˆå¦‚â€œbad batteryâ€ï¼‰\n\nTopic Ã— Sentiment:\n\nå…ˆè¯†åˆ«å¥å­çš„ä¸»é¢˜ï¼ˆSC-LDAï¼‰\nå¯¹å¥å­åšæƒ…ç»ªåˆ†ç±»\nå°†æƒ…ç»ªä¸ä¸»é¢˜è¿›è¡Œäº¤äº’å»ºæ¨¡ â†’ æ–¹é¢çº§æƒ…æ„Ÿåˆ†æï¼ˆAspect-Based Sentiment Analysisï¼‰"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#summary",
    "href": "02_topic_sentiment/lecture2.html#summary",
    "title": "Lecture 2",
    "section": "Summary",
    "text": "Summary\n\n\n\nä¸»é¢˜\nå†…å®¹\n\n\n\n\nLDA æ ¸å¿ƒ\nä¸»é¢˜åˆ†å¸ƒ Ã— è¯åˆ†å¸ƒï¼ˆÎ¸ Ã— Î²ï¼‰\n\n\næ¨æ–­æ–¹æ³•\nVIï¼ˆå¿«ï¼‰ vs MCMCï¼ˆå‡†ï¼‰\n\n\næ¨¡å‹ç»“æ„\næ–‡æ¡£ = å¤šä¸»é¢˜ï¼Œè¯ = ä»ä¸»é¢˜ä¸­æŠ½æ ·\n\n\nå˜ä½“\nCTM, Dynamic LDA, Author-Topic, SC-LDA\n\n\nè¿›é˜¶æ–¹å‘\nåŠ å…¥æƒ…ç»ªã€æ—¶é—´ã€ç»“æ„åŒ–å…ˆéªŒ"
  },
  {
    "objectID": "02_topic_sentiment/lecture2.html#references",
    "href": "02_topic_sentiment/lecture2.html#references",
    "title": "Lecture 2",
    "section": "References",
    "text": "References\n\nHu, M., & Liu, B. (2004). Mining and summarizing customer reviews.\nAgarwal, A., & Mittal, N. (2015). Text classification using machine learning methods.\nWordNet: https://wordnet.princeton.edu/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nlp-notes",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]