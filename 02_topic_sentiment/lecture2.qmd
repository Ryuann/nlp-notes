---
title: "Lecture 2"
format: html
---
---

## Language Modeling and Probability Foundations

### Sequence Modeling and Prediction

Language models predict the next word in a sequence given the preceding words. This is typically modeled using conditional probability:

è¯­è¨€æ¨¡å‹çš„åŸºæœ¬ä»»åŠ¡æ˜¯ï¼š**æ ¹æ®å‰é¢çš„è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯**ã€‚ç”¨Conditional Probabilityè¡¨ç¤ºï¼š

$$
P(w_5 \mid w_1, w_2, w_3, w_4)
$$

This expresses the probability of the 5th word given the first four.

#### Example:

Sentence: *"Luck is what happens when preparation meets opportunity."*

åœ¨â€œLuck is what happensâ€ä¹‹åå‡ºç°â€œwhenâ€çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ

### Chain Rule of Probability

To compute complex conditional probabilities, we use the **chain rule**:

ä¸ºäº†è®¡ç®—å¤æ‚çš„æ¡ä»¶æ¦‚ç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¦‚ç‡çš„Chain Ruleï¼š

* Basic rule:
  $P(A \cap B) = P(A) P(B|A)$
* Conditional probability definition:
  $P(B|A) = \frac{P(A \cap B)}{P(A)}$
* Generalized form:
  $P(A \cap B \cap C) = P(C|B \cap A) P(B|A) P(A)$

In the context of language:

ç›®æ ‡æ˜¯ä¼°è®¡ï¼š

$$
P(\text{when} \mid \text{happens}, \text{what}, \text{is}, \text{luck})
$$

ä½¿ç”¨chain ruleå±•å¼€ï¼š

$$
P(\text{when}) = P(\text{when} \mid \text{happens} \cap \text{what} \cap \text{is} \cap \text{luck}) \\
\cdot P(\text{happens} \mid \text{what} \cap \text{is} \cap \text{luck}) \\
\cdot P(\text{what} \mid \text{is} \cap \text{luck}) \\
\cdot P(\text{is} \mid \text{luck}) \\
\cdot P(\text{luck})
$$

è¿™å°±æ˜¯è¯­è¨€æ¨¡å‹ä¸­å¸¸ç”¨çš„æ€è·¯ï¼šå°†ä¸€ä¸ªå¥å­ä¸­å„ä¸ªè¯çš„è”åˆæ¦‚ç‡è½¬åŒ–ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯ã€‚

---

## Perplexity and Cross-Entropy

### Perplexity

Perplexity evaluates how well a probability model predicts a sample:

> **Perplexity = 2 raised to the cross-entropy of the model**

Perplexityè¡¡é‡çš„æ˜¯æ¨¡å‹å¯¹æµ‹è¯•é›†çš„â€œå›°æƒ‘â€ç¨‹åº¦ï¼Œ**è¶Šå°è¶Šå¥½**ã€‚

å®ƒç­‰ä»·äºæ¨¡å‹æ¯é¢„æµ‹ä¸€ä¸ªè¯æ—¶çš„ä¸ç¡®å®šåº¦ã€‚

$$
\text{Perplexity} = 2^{-\frac{1}{W} \sum_{k=1}^{n} \log P(w_k)}
$$

å…¶ä¸­ï¼š

* $W$ï¼šæµ‹è¯•é›†ä¸­å•è¯æ€»æ•°
* $P(w_k)$ï¼šæ¨¡å‹ç»™ç¬¬ $k$ ä¸ªè¯çš„æ¦‚ç‡é¢„æµ‹

### Cross-Entropy

Cross-entropy quantifies the average number of bits needed to encode data from a distribution $\tilde{p}$ using a model $q$:

äº¤å‰ç†µï¼ˆCross-Entropyï¼‰ç”¨æ¥è¡¡é‡ä¸€ä¸ªåˆ†å¸ƒ $\tilde{p}$ å’Œæ¨¡å‹åˆ†å¸ƒ $q$ ä¹‹é—´çš„è·ç¦»ï¼š

$$
H(\tilde{p}, q) = -\sum_{i=1}^N \tilde{p}(x_i) \log_2 q(x_i)
$$

å…¶ä¸­ï¼š

* $\tilde{p}(x_i)$ï¼šç»éªŒæ¦‚ç‡ï¼ˆè¯ $x_i$ çš„å‡ºç°é¢‘ç‡ï¼‰
* $q(x_i)$ï¼šæ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡

å’Œ Perplexity çš„å…³ç³»ï¼š

$$
\text{Perplexity} = 2^{\text{Cross-Entropy}}
$$

### KL Divergence (Relative Entropy)

KL æ•£åº¦è¡¡é‡ä¸€ä¸ªè¿‘ä¼¼åˆ†å¸ƒ $q$ ä¸çœŸå®åˆ†å¸ƒ $p$ çš„å·®å¼‚ã€‚å®ƒä¸å¯¹ç§°ï¼Œä½†å¦‚æœä¸¤è€…ç›¸ç­‰ï¼ŒKL æ•£åº¦ä¸º 0ã€‚

å…¬å¼ï¼š

$$
D_{KL}(p \parallel q) = \sum_{i=1}^N p(x_i) \log \left( \frac{p(x_i)}{q(x_i)} \right)
$$

* $p(x_i)$ï¼šçœŸå®åˆ†å¸ƒ
* $q(x_i)$ï¼šæ¨¡å‹çš„è¿‘ä¼¼åˆ†å¸ƒ

ç”¨é€”ï¼šKL æ•£åº¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸ç”¨äºä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œæ¯”å¦‚é€šè¿‡æœ€å°åŒ– KL æ•£åº¦æ¥é€¼è¿‘çœŸå®åˆ†å¸ƒã€‚

---

## Text Representation Models

### Bag-of-Words (BoW)

Converts text into vectors based on word occurrence counts.

BoWï¼ˆè¯è¢‹æ¨¡å‹ï¼‰å°†æ–‡æœ¬å‘é‡åŒ–ï¼Œæ¯ä¸ªç»´åº¦è¡¨ç¤ºè¯è¡¨ä¸­ä¸€ä¸ªè¯åœ¨å¥å­ä¸­å‡ºç°çš„é¢‘ç‡ã€‚

#### ä¾‹å­è¯´æ˜ï¼š

å¥å­ï¼š

```
I WOULD NOT, COULD NOT IN THE RAIN.
NOT IN THE DARK. NOT ON A TRAIN.
```

è¢«è½¬æ¢ä¸ºä¸¤ä¸ªå‘é‡ï¼š

```
[1, 1, 1, 2, 1, 1, 1, 1, 0, 0, ..., 0]
[0, 0, 0, 2, 1, 1, 0, 1, 1, 1, ..., 0]
```

æ¯ä¸ªæ•°è¡¨ç¤ºè¯è¡¨ä¸­å¯¹åº”å•è¯åœ¨è¯¥â€œæ–‡æ¡£â€ï¼ˆå³å¥å­ï¼‰ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚

### Limitations of BoW

BoW çš„ä¸»è¦ç¼ºç‚¹ï¼š

* **ä¸è€ƒè™‘è¯­ä¹‰ï¼ˆSemantic meaningï¼‰**ï¼šå¿½ç•¥ä¸Šä¸‹æ–‡ï¼Œæ¯”å¦‚â€œæˆ‘çˆ±ä½ â€å’Œâ€œä½ çˆ±æˆ‘â€ä¼šè¢«è§†ä¸ºç›¸åŒå‘é‡ã€‚
* **ç»´åº¦é«˜ï¼ˆVector sizeï¼‰**ï¼šè¯è¡¨å¤§æ—¶ï¼Œå‘é‡ç¨€ç–ã€å ç”¨èµ„æºå¤šã€‚

### TF-IDF (Term Frequency-Inverse Document Frequency)

Gives less importance to common words:

ç›®çš„ï¼šé™ä½é«˜é¢‘æ— æ„ä¹‰è¯ï¼ˆå¦‚ â€œtheâ€, â€œandâ€ï¼‰çš„æƒé‡ï¼Œæé«˜æœ‰åŒºåˆ†åº¦çš„è¯çš„é‡è¦æ€§ã€‚

å®šä¹‰ï¼š

* **TFï¼ˆTerm Frequencyï¼‰**ï¼šè¯åœ¨æ–‡æ¡£ä¸­çš„é¢‘ç‡ï¼›
* **IDFï¼ˆInverse Document Frequencyï¼‰**ï¼šè¯åœ¨æ•´ä¸ªæ–‡æ¡£é›†ä¸­å‡ºç°çš„â€œé€†é¢‘ç‡â€ã€‚

å…¬å¼ï¼š

$$
\text{TF-IDF}(t, d) = tf(t, d) \cdot \log \left( \frac{N}{df(t)} \right)
$$

å…¶ä¸­ï¼š

* $t$ï¼šè¯
* $d$ï¼šæ–‡æ¡£
* $df(t)$ï¼šåŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°
* $N$ï¼šæ–‡æ¡£æ€»æ•°

### Normalized Term Frequency

ä¸ºäº†è§£å†³æ–‡æ¡£é•¿çŸ­å¯¹è¯é¢‘å¸¦æ¥çš„åå·®ï¼Œå¯ä»¥è¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼š

* **å¯¹æ•°ç¼©æ”¾ï¼ˆLog Normalizationï¼‰**ï¼š
  $\log(1 + f_{t,d})$
* **æœ€å¤§å€¼å½’ä¸€åŒ–ï¼ˆMax Normalizationï¼‰**ï¼š

  $$
  tf(t,d) = 0.5 + 0.5 \cdot \frac{f_{t,d}}{\max \{f_{t',d} : t' \in d\}}
  $$

---

## N-gram Models

### Bi-grams and N-grams

Bi-gramï¼ˆåŒè¯æ¨¡å‹ï¼‰è§£å†³ BoW å¿½ç•¥è¯åºçš„é—®é¢˜ï¼Œå°†æ¯ä¸¤ä¸ªè¯ä½œä¸ºä¸€ä¸ªå•å…ƒè¿›è¡Œå»ºæ¨¡ã€‚

ä¾‹å¦‚ï¼š

```
"Luck is", "is what", "what happens", ...
```

### N-gram ä¸€èˆ¬åŒ–

N-gram æ˜¯å¯¹ Bi-gram çš„æ‹“å±•ï¼Œä½¿ç”¨å‰ $N-1$ ä¸ªè¯é¢„æµ‹å½“å‰è¯ã€‚

#### æ•°å­¦è¡¨è¾¾å¼ï¼š

* Unigram:
  $P(w_i) = \frac{\text{Count}(w_i)}{\sum_j \text{Count}(w_j)}$
* Bigram:
  $P(w_i | w_{i-1}) = \frac{\text{Count}(w_{i-1}, w_i)}{\text{Count}(w_{i-1})}$
* N-gram:
  $P(w_1^n) = \prod_{k=1}^{n} P(w_k | w_{k-1})$

è¡¨ç¤ºæ•´å¥è¯çš„æ¦‚ç‡æ˜¯å„ä¸ªè¯çš„æ¡ä»¶æ¦‚ç‡è¿ä¹˜ã€‚

### Out-of-Vocabulary Wordsï¼ˆè¯è¡¨å¤–è¯ï¼‰

å½“è®­ç»ƒé›†ä¸­æ²¡æœ‰å‡ºç°çš„è¯å« OOVï¼ˆOut-of-Vocabularyï¼‰ï¼Œè¿™ä¼šå¯¼è‡´æ¦‚ç‡ä¸ºé›¶ã€‚

è§£å†³åŠæ³•ï¼š

* æ‰©å¤§è¯­æ–™åº“ï¼ˆIncrease corpus sizeï¼‰
* è·³è¿‡ç¼ºå¤±çš„ n-gramï¼ˆLeap over missing n-gramsï¼‰
* ä½¿ç”¨å¹³æ»‘æŠ€æœ¯ï¼ˆSmoothingï¼‰ï¼Œå¦‚ï¼š
  $P_{\text{Laplace}}(w_i) = \frac{c_i + 1}{N + V}$
  å…¶ä¸­ï¼š

  * $c_i$ï¼šè¯ $w_i$ å‡ºç°æ¬¡æ•°
  * $N$ï¼šè¯æ€»æ•°
  * $V$ï¼šè¯è¡¨å¤§å°

---

## Sentiment Analysis and Semantic Orientation

### Definition

Semantic orientation analyzes sentiment strength and direction in text.

è¯­ä¹‰å€¾å‘ï¼ˆSemantic Orientationï¼‰ç”¨äºåˆ¤æ–­æ–‡æœ¬ä¸­å•è¯æˆ–çŸ­è¯­çš„æƒ…ç»ªææ€§å’Œå¼ºåº¦ã€‚

#### Key dimensions:

* **Subjectivity**: objective vs subjective ï¼ˆå®¢è§‚ vs ä¸»è§‚ï¼‰
* **Polarity**: negative vs positive ï¼ˆè´Ÿé¢ vs æ­£é¢ï¼‰
* **Intensity**: strength (e.g. "very good" vs "good")

### Lexicon-Based Approach

åŸºäºäººå·¥æˆ–åŠè‡ªåŠ¨æ„å»ºçš„æƒ…æ„Ÿè¯å…¸ï¼Œå¦‚ Hu & Liuï¼ˆ2004ï¼‰çš„æ–¹æ³•ï¼š

* æå– **æ„è§è¯ï¼ˆopinion wordsï¼‰**ï¼Œå¦‚å½¢å®¹è¯ï¼ˆgood, bad, amazingï¼‰ï¼›
* ä½¿ç”¨ WordNet è·å–åŒä¹‰è¯/åä¹‰è¯ï¼Œæ¨æ–­å…¶æƒ…ç»ªå€¾å‘ï¼›
* é€è¯åˆ†æå¹¶åˆå¹¶ä¸ºå¥å­çº§åˆ«åˆ¤æ–­ã€‚

### WordNet Role

WordNet æ˜¯ä¸€ä¸ªè¯æ±‡çŸ¥è¯†åº“ï¼Œå®ƒç”¨è¯­ä¹‰å…³ç³»è¿æ¥å•è¯ï¼ŒåŒ…æ‹¬ï¼š

* Synonymï¼ˆåŒä¹‰è¯ï¼‰ï¼šå½¢æˆåŒä¹‰é›† Synsets
* Antonymï¼ˆåä¹‰è¯ï¼‰ï¼šå¦‚ wet ä¸ dry
* ä¸Šä¸‹ä½å…³ç³»ï¼ˆHypernym-Hyponymï¼‰ï¼šå¦‚ furniture > bed
* éƒ¨åˆ†æ•´ä½“å…³ç³»ï¼ˆMeronymï¼‰ï¼šå¦‚ chair ä¸ backrest

### Limitations of Lexicon Methods

å¼•ç”¨è‡ª Agarwal & Mittalï¼ˆ2015ï¼‰ï¼š

ä¸»è¦é—®é¢˜ï¼š

* ä¾èµ–è®­ç»ƒè¯­æ–™åº“ä¸­çš„å·²æœ‰è¯æ±‡ï¼›
* å¯¹æ–°è¯ã€ä¿šè¯­ã€ç‰¹å®šé¢†åŸŸæœ¯è¯­æ— èƒ½ä¸ºåŠ›ï¼›
* è‹¥è¯å…¸è¦†ç›–ä¸å…¨ï¼Œç³»ç»Ÿå®¹æ˜“å¤±è¯¯ï¼›
* æ–‡æœ¬é•¿åº¦è¶ŠçŸ­ï¼Œè¯­ä¹‰è¶Šéš¾åˆ¤æ–­å‡†ç¡®ã€‚

---

## Summary Table

| Topic             | Key Concept                  | Limitation                 |
| ----------------- | ---------------------------- | -------------------------- |
| Language Modeling | Predict next word            | Sensitive to data sparsity |
| Perplexity        | Measures model's confusion   | Lower = better             |
| BoW               | Simple vector representation | Ignores word order         |
| TF-IDF            | Penalizes common words       | Still context-free         |
| N-gram            | Uses local context           | Struggles with OOV words   |
| Lexicon Sentiment | Dictionary-based polarity    | Poor generalization        |

---


## What Are Topic Models?

Topic models are statistical methods that discover abstract "topics" in a collection of documents. The most well-known model is **Latent Dirichlet Allocation (LDA)**. Documents are modeled as mixtures of topics, and each topic is characterized by a distribution over words.

æ˜¯ä¸€ç±»æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬é›†åˆä¸­è‡ªåŠ¨å‘ç°**æ½œåœ¨è¯é¢˜**ã€‚

* doc = å¤šä¸ªä¸»é¢˜çš„æ··åˆï¼›
* topic = ä¸€ç»„è¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚

---

## LDA Graphical Structure & Plate Notation

* Î±ï¼šæ§åˆ¶æ–‡æ¡£ä¸­ä¸»é¢˜åˆ†å¸ƒçš„ Dirichlet å…ˆéªŒ
* Î¸\_dï¼šç¬¬ d ç¯‡æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒ
* z\_dnï¼šç¬¬ d ç¯‡æ–‡æ¡£ä¸­ç¬¬ n ä¸ªè¯çš„ä¸»é¢˜
* Î²\_kï¼šç¬¬ k ä¸ªä¸»é¢˜ä¸‹çš„è¯åˆ†å¸ƒ
* w\_dnï¼šå®é™…ç”Ÿæˆçš„è¯
* Î·ï¼šæ§åˆ¶ Î² çš„ Dirichlet è¶…å‚æ•°

ğŸ§¾ **ç”Ÿæˆè¿‡ç¨‹**ï¼š

1. å¯¹æ¯ç¯‡æ–‡æ¡£ dï¼Œä» Dirichlet(Î±) æŠ½æ ·å‡º Î¸\_d
2. å¯¹æ¯ä¸ªè¯ï¼Œä» Multinomial(Î¸\_d) é€‰æ‹©ä¸»é¢˜ z\_dn
3. å†ä» Multinomial(Î²\_{z\_dn}) ä¸­é‡‡æ ·å‡ºè¯ w\_dn

---

## LDA as Soft Clustering

LDA æ˜¯ä¸€ç§Soft Clusteringï¼‰
* Each word is probabilistically assigned to a topic.
* Unlike hard clustering (e.g. K-means), LDA allows multiple topics per document.

* æ¯ä¸ªè¯æ˜¯ä»å¤šä¸ªä¸»é¢˜ä¸­ä»¥æ¦‚ç‡æ–¹å¼æŠ½æ ·å¾—åˆ°çš„ï¼›
* æ¯ä¸ªæ–‡æ¡£å¯èƒ½å±äºå¤šä¸ªä¸»é¢˜ï¼›
* åŒºåˆ«äº Latent Class æ¨¡å‹ï¼ˆç¡¬èšç±»ï¼Œæ¯ä¸ªæ–‡æ¡£åªå±äºä¸€ä¸ªä¸»é¢˜ï¼‰ã€‚

---

## LDA as Dimension Reduction
LDA ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§**é™ç»´æŠ€æœ¯**ï¼š
* æŠŠé«˜ç»´çš„è¯é¢‘å‘é‡æ˜ å°„ä¸ºä½ç»´çš„ä¸»é¢˜ç©ºé—´ï¼›
* ä¸ PCA ç±»ä¼¼ï¼Œä½†ï¼š
  * LDA çš„åˆ†é‡æ˜¯æ­£æ•°
  * ç”¨æ¦‚ç‡å»ºæ¨¡è€Œéçº¿æ€§æŠ•å½±

ğŸ“Œ æ›¿ä»£æ–¹æ¡ˆï¼šçŸ©é˜µåˆ†è§£ï¼ˆMatrix Factorizationï¼‰ã€äº¤æ›¿æœ€å°äºŒä¹˜ï¼ˆALSï¼‰
* Expected word vector: $\mathbb{E}[w] = \theta^\top \beta$
* Î¸: document's topic distribution
* Î²: topic's word distribution

---

## Variational Inference in LDA

**Why Variational Inference (VI)?**

* Posterior $p(\theta, z \mid w)$ is intractable.
* VI approximates it with simpler $q(\theta, z)$ by minimizing KL divergence.

**VI Mechanism:**

* Use mean-field factorization: $q(\theta, z) = q(\theta) \prod_n q(z_n)$
* Optimize parameters $\gamma, \phi$
* Iteratively update:

  * $\phi_{ni} \propto \beta_{i w_n} \exp(\Psi(\gamma_i))$
  * $\gamma_i = \alpha_i + \sum_n \phi_{ni}$


* è´å¶æ–¯æ¨æ–­è¦æ±‚è®¡ç®—åéªŒåˆ†å¸ƒï¼Œä½†ä¸å¯è§£æï¼›
* VI æŠŠæ¨æ–­é—®é¢˜è½¬ä¸ºä¼˜åŒ–é—®é¢˜ï¼š

  * é€‰ä¸€ä¸ªå¯å¤„ç†çš„åˆ†å¸ƒæ— $q$ï¼Œä½¿å…¶æœ€æ¥è¿‘çœŸå®åéªŒã€‚
  * ç”¨å˜åˆ†å‚æ•° $\gamma$ å’Œ $\phi$ è¡¨è¾¾æ–‡æ¡£çš„ä¸»é¢˜è¡¨ç¤ºå’Œè¯çš„ä¸»é¢˜åˆ†å¸ƒã€‚

**æ¯”è¾ƒï¼š**

| æ–¹æ³•   | ä¼˜ç‚¹      | ç¼ºç‚¹        |
| ---- | ------- | --------- |
| MCMC | ç²¾åº¦é«˜     | æ…¢ï¼Œä¸é€‚åˆå¤§è§„æ¨¡  |
| VI   | å¿«ï¼Œæœ‰è§£æå½¢å¼ | æœ‰åå·®ï¼Œä¾èµ–åˆ†å¸ƒæ— |

---

## Extensions of LDA

| æ‰©å±•æ¨¡å‹                        | æè¿°                         |
| --------------------------- | -------------------------- |
| CTMï¼ˆCorrelated Topic Modelï¼‰ | ä½¿ç”¨ Logistic Normal å»ºæ¨¡ä¸»é¢˜ç›¸å…³æ€§ |
| Author-Topic Model          | æ¯ä¸ªä½œè€…æ‹¥æœ‰ä¸€ä¸ªä¸»é¢˜åˆ†å¸ƒ               |
| Dynamic Topic Model         | æ¨¡å‹ä¸­ä¸»é¢˜ä¼šéšæ—¶é—´å˜åŒ–                |

ğŸ“Œ ç¼ºç‚¹ï¼šè¿™äº›æ‰©å±•å¾€å¾€ç ´å Dirichlet çš„ç®€æ´æ€§è´¨ï¼Œä½¿å¾—æ¨æ–­å˜å¾—æ›´å¤æ‚ï¼ˆé€šå¸¸éœ€è¦ MCMC æˆ– EPï¼‰

---

## Sentence-Constrained LDA

SC-LDA å‡è®¾ï¼š**ä¸€å¥è¯é€šå¸¸åªè®²ä¸€ä¸ªä¸»é¢˜**ï¼Œå¹¶ä¸”è¿™ä¸ªä¸»é¢˜å¯èƒ½åœ¨ä¸‹ä¸€å¥ä¸­å»¶ç»­ã€‚

* æ¯å¥ â†’ ä¸€ä¸ªä¸»é¢˜
* æ›´è´´è¿‘ç”¨æˆ·åœ¨è¯„è®ºä¸­çš„è¡¨è¾¾ç»“æ„
* å¯ç”¨äºäº§å“å±æ€§æå–ï¼ˆå¦‚â€œå±å¹•ã€ä»·æ ¼ã€ç”µæ± â€ï¼‰
* æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒå¯ç”¨äº**é¢„æµ‹è¯„åˆ†**
* Each sentence tends to express one topic (Buschken & Allenby, 2016)
* Better topic-word tail modeling
* Improves interpretability & rating prediction

### æƒ…æ„Ÿä¸æ˜¯ä¸»é¢˜ï¼š

* ä¸»é¢˜ç”±â€œåè¯â€é©±åŠ¨ï¼Œæƒ…æ„Ÿç”±â€œå½¢å®¹è¯â€é©±åŠ¨ï¼›
* åŒä¸€ä¸ªä¸»é¢˜å¯èƒ½åŒ…å«è¤’è´¬ä¸ä¸€çš„å†…å®¹ï¼›
* åœ¨å¤§æ•°æ®ä¸­å¯èƒ½å­¦å‡ºâ€œä¸»é¢˜+æƒ…ç»ªâ€å¤åˆç»“æ„ï¼ˆå¦‚â€œbad batteryâ€ï¼‰

Topic Ã— Sentiment: 

1. å…ˆè¯†åˆ«å¥å­çš„ä¸»é¢˜ï¼ˆSC-LDAï¼‰
2. å¯¹å¥å­åšæƒ…ç»ªåˆ†ç±»
3. å°†æƒ…ç»ªä¸ä¸»é¢˜è¿›è¡Œäº¤äº’å»ºæ¨¡ â†’ æ–¹é¢çº§æƒ…æ„Ÿåˆ†æï¼ˆAspect-Based Sentiment Analysisï¼‰

---

## Summary

| ä¸»é¢˜     | å†…å®¹                                     |
| ------ | -------------------------------------- |
| LDA æ ¸å¿ƒ | ä¸»é¢˜åˆ†å¸ƒ Ã— è¯åˆ†å¸ƒï¼ˆÎ¸ Ã— Î²ï¼‰                      |
| æ¨æ–­æ–¹æ³•   | VIï¼ˆå¿«ï¼‰ vs MCMCï¼ˆå‡†ï¼‰                       |
| æ¨¡å‹ç»“æ„   | æ–‡æ¡£ = å¤šä¸»é¢˜ï¼Œè¯ = ä»ä¸»é¢˜ä¸­æŠ½æ ·                    |
| å˜ä½“     | CTM, Dynamic LDA, Author-Topic, SC-LDA |
| è¿›é˜¶æ–¹å‘   | åŠ å…¥æƒ…ç»ªã€æ—¶é—´ã€ç»“æ„åŒ–å…ˆéªŒ                          |

## References

* Hu, M., & Liu, B. (2004). Mining and summarizing customer reviews.
* Agarwal, A., & Mittal, N. (2015). Text classification using machine learning methods.
* WordNet: [https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)
