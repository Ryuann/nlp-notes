---
title: "Video Notes: Ngram, language model, sentiment analysis"
format: html
---
---

## Language Modeling and Probability Foundations

### Sequence Modeling and Prediction

Language models predict the next word in a sequence given the preceding words. This is typically modeled using conditional probability:

语言模型的基本任务是：**根据前面的词来预测下一个词**。通常使用\*\*条件概率（Conditional Probability）\*\*来表示：

$$
P(w_5 \mid w_1, w_2, w_3, w_4)
$$

This expresses the probability of the 5th word given the first four.

#### Example:

Sentence: *"Luck is what happens when preparation meets opportunity."*

我们可以问：**在“Luck is what happens”之后出现“when”的概率是多少？**

### Chain Rule of Probability

To compute complex conditional probabilities, we use the **chain rule**:

为了计算复杂的条件概率，我们使用**概率的链式法则（Chain Rule）**：

* Basic rule:
  $P(A \cap B) = P(A) P(B|A)$
* Conditional probability definition:
  $P(B|A) = \frac{P(A \cap B)}{P(A)}$
* Generalized form:
  $P(A \cap B \cap C) = P(C|B \cap A) P(B|A) P(A)$

In the context of language:

目标是估计：

$$
P(\text{when} \mid \text{happens}, \text{what}, \text{is}, \text{luck})
$$

使用链式法则展开：

$$
P(\text{when}) = P(\text{when} \mid \text{happens} \cap \text{what} \cap \text{is} \cap \text{luck}) \\
\cdot P(\text{happens} \mid \text{what} \cap \text{is} \cap \text{luck}) \\
\cdot P(\text{what} \mid \text{is} \cap \text{luck}) \\
\cdot P(\text{is} \mid \text{luck}) \\
\cdot P(\text{luck})
$$

这就是语言模型中常用的思路：将一个句子中各个词的联合概率转化为一系列条件概率的乘积。

---

## Perplexity and Cross-Entropy

### Perplexity

Perplexity evaluates how well a probability model predicts a sample:

> **Perplexity = 2 raised to the cross-entropy of the model**

Perplexity（困惑度）衡量的是模型对测试集的“困惑”程度，**越小越好**。

它等价于模型每预测一个词时的不确定度。

$$
\text{Perplexity} = 2^{-\frac{1}{W} \sum_{k=1}^{n} \log P(w_k)}
$$

其中：

* $W$：测试集中单词总数
* $P(w_k)$：模型给第 $k$ 个词的概率预测

### Cross-Entropy

Cross-entropy quantifies the average number of bits needed to encode data from a distribution $\tilde{p}$ using a model $q$:

交叉熵（Cross-Entropy）用来衡量一个分布 $\tilde{p}$ 和模型分布 $q$ 之间的距离：

$$
H(\tilde{p}, q) = -\sum_{i=1}^N \tilde{p}(x_i) \log_2 q(x_i)
$$

其中：

* $\tilde{p}(x_i)$：经验概率（词 $x_i$ 的出现频率）
* $q(x_i)$：模型预测的概率

和 Perplexity 的关系：

$$
\text{Perplexity} = 2^{\text{Cross-Entropy}}
$$

### KL Divergence (Relative Entropy)

KL 散度衡量一个近似分布 $q$ 与真实分布 $p$ 的差异。它不对称，但如果两者相等，KL 散度为 0。

公式：

$$
D_{KL}(p \parallel q) = \sum_{i=1}^N p(x_i) \log \left( \frac{p(x_i)}{q(x_i)} \right)
$$

* $p(x_i)$：真实分布
* $q(x_i)$：模型的近似分布

用途：KL 散度在训练过程中常用于优化目标函数，比如通过最小化 KL 散度来逼近真实分布。

---

## Text Representation Models

### Bag-of-Words (BoW)

Converts text into vectors based on word occurrence counts.

BoW（词袋模型）将文本向量化，每个维度表示词表中一个词在句子中出现的频率。

#### 例子说明：

句子：

```
I WOULD NOT, COULD NOT IN THE RAIN.
NOT IN THE DARK. NOT ON A TRAIN.
```

被转换为两个向量：

```
[1, 1, 1, 2, 1, 1, 1, 1, 0, 0, ..., 0]
[0, 0, 0, 2, 1, 1, 0, 1, 1, 1, ..., 0]
```

每个数表示词表中对应单词在该“文档”（即句子）中出现的次数。

### Limitations of BoW

BoW 的主要缺点：

* **不考虑语义（Semantic meaning）**：忽略上下文，比如“我爱你”和“你爱我”会被视为相同向量。
* **维度高（Vector size）**：词表大时，向量稀疏、占用资源多。

### TF-IDF (Term Frequency-Inverse Document Frequency)

Gives less importance to common words:

目的：降低高频无意义词（如 “the”, “and”）的权重，提高有区分度的词的重要性。

定义：

* **TF（Term Frequency）**：词在文档中的频率；
* **IDF（Inverse Document Frequency）**：词在整个文档集中出现的“逆频率”。

公式：

$$
\text{TF-IDF}(t, d) = tf(t, d) \cdot \log \left( \frac{N}{df(t)} \right)
$$

其中：

* $t$：词
* $d$：文档
* $df(t)$：包含该词的文档数
* $N$：文档总数

### Normalized Term Frequency

为了解决文档长短对词频带来的偏差，可以进行规范化处理：

* **对数缩放（Log Normalization）**：
  $\log(1 + f_{t,d})$
* **最大值归一化（Max Normalization）**：

  $$
  tf(t,d) = 0.5 + 0.5 \cdot \frac{f_{t,d}}{\max \{f_{t',d} : t' \in d\}}
  $$

---

## N-gram Models

### Bi-grams and N-grams

Bi-gram（双词模型）解决 BoW 忽略词序的问题，将每两个词作为一个单元进行建模。

例如：

```
"Luck is", "is what", "what happens", ...
```

### N-gram 一般化

N-gram 是对 Bi-gram 的拓展，使用前 $N-1$ 个词预测当前词。

#### 数学表达式：

* Unigram:
  $P(w_i) = \frac{\text{Count}(w_i)}{\sum_j \text{Count}(w_j)}$
* Bigram:
  $P(w_i | w_{i-1}) = \frac{\text{Count}(w_{i-1}, w_i)}{\text{Count}(w_{i-1})}$
* N-gram:
  $P(w_1^n) = \prod_{k=1}^{n} P(w_k | w_{k-1})$

表示整句话的概率是各个词的条件概率连乘。

### Out-of-Vocabulary Words（词表外词）

当训练集中没有出现的词叫 OOV（Out-of-Vocabulary），这会导致概率为零。

解决办法：

* 扩大语料库（Increase corpus size）
* 跳过缺失的 n-gram（Leap over missing n-grams）
* 使用平滑技术（Smoothing），如：
  $P_{\text{Laplace}}(w_i) = \frac{c_i + 1}{N + V}$
  其中：

  * $c_i$：词 $w_i$ 出现次数
  * $N$：词总数
  * $V$：词表大小

---

## Sentiment Analysis and Semantic Orientation

### Definition

Semantic orientation analyzes sentiment strength and direction in text.

语义倾向（Semantic Orientation）用于判断文本中单词或短语的情绪极性和强度。

#### Key dimensions:

* **Subjectivity**: objective vs subjective （客观 vs 主观）
* **Polarity**: negative vs positive （负面 vs 正面）
* **Intensity**: strength (e.g. "very good" vs "good")

### Lexicon-Based Approach

基于人工或半自动构建的情感词典，如 Hu & Liu（2004）的方法：

* 提取 **意见词（opinion words）**，如形容词（good, bad, amazing）；
* 使用 WordNet 获取同义词/反义词，推断其情绪倾向；
* 逐词分析并合并为句子级别判断。

### WordNet Role

WordNet 是一个词汇知识库，它用语义关系连接单词，包括：

* Synonym（同义词）：形成同义集 Synsets
* Antonym（反义词）：如 wet 与 dry
* 上下位关系（Hypernym-Hyponym）：如 furniture > bed
* 部分整体关系（Meronym）：如 chair 与 backrest

### Limitations of Lexicon Methods

引用自 Agarwal & Mittal（2015）：

主要问题：

* 依赖训练语料库中的已有词汇；
* 对新词、俚语、特定领域术语无能为力；
* 若词典覆盖不全，系统容易失误；
* 文本长度越短，语义越难判断准确。

---

## Summary Table

| Topic             | Key Concept                  | Limitation                 |
| ----------------- | ---------------------------- | -------------------------- |
| Language Modeling | Predict next word            | Sensitive to data sparsity |
| Perplexity        | Measures model's confusion   | Lower = better             |
| BoW               | Simple vector representation | Ignores word order         |
| TF-IDF            | Penalizes common words       | Still context-free         |
| N-gram            | Uses local context           | Struggles with OOV words   |
| Lexicon Sentiment | Dictionary-based polarity    | Poor generalization        |

---

## References

* Hu, M., & Liu, B. (2004). Mining and summarizing customer reviews.
* Agarwal, A., & Mittal, N. (2015). Text classification using machine learning methods.
* WordNet: [https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)
