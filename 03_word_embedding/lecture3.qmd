## BoW

每个词就是一个维度，一个文本中是否出现这个词就用 1 或 0 表示。  
词汇表是 [apple, banana, cat]，句子 “apple cat” 就表示为 `[1, 0, 1]`。因为词表大，每个文本在高维空间中只激活少数几个词，导致大部分维度为 0，即“稀疏向量（sparse vector）”。

常见做法是使用 one-hot encoding，使得词向量之间是正交的，即没有语义上的关联。比如 apple 是 `[1,0,0]`，banana 是 `[0,1,0]`，所以它们之间的余弦相似度为 0，无法反映“apple”和“banana”都属于“水果”这个语义信息。

## TF-IDF 是 BoW 的加权版本

TF-IDF（词频-逆文档频率）给词语赋予权重，强调词语在当前文本中频繁出现、但在其他文本中少见的词（如“癌症”、“利润”）更重要。

## Pointwise Mutual Information (PMI)

$$
\text{PMI}(w, d) = \log \frac{P(w, d)}{P(w)P(d)}
$$

衡量一个词出现在某个文档中的概率是否超过随机概率。

## KL 散度（KL Divergence）

度量两个分布之间的差异：

$$
KL(P \parallel Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$

比如 $P$ 是某词的真实分布，$Q$ 是基线（如平均分布），它衡量“信息量”或“惊奇程度”。

TF-IDF 的作用和这些信息论指标一致，都是为了突出“稀有但有用的词”。

## BoW 的三大缺陷

1. **新词（Out-of-Vocabulary）无法处理**：除非手动添加到词汇表中。
2. **形成稀疏矩阵（sparse matrix）**：大量词不会出现在某些句子中，导致向量中大多为 0。
3. **Word ordering & grammar 丢失**：“dog bites man” 和 “man bites dog” 语序不同，但 BoW 认为它们完全一样。

## Distributional Assumption

语言学家 John Firth 提出：

> "You shall know a word by the company it keeps."

**分布式假设（Distributional Hypothesis）**指出：两个词若语义相似，它们在语料中的上下文也相似。

这为 Word2Vec、GloVe 等词向量模型奠定理论基础。

## TF-IDF 与信息论（Information Theory）

我们用互信息（Mutual Information）来解释 TF-IDF 的信息作用：

$$
I(D; W) = H(D) - H(D \mid W) = \sum_{w_i \in W} P(w_i) \left[ H(D) - H(D \mid w_i) \right]
$$

替换为频数形式：

$$
= \sum_{w_i \in W} \frac{f_{w_i}}{F} \left( -\log \frac{1}{N} + \log \frac{1}{N_i} \right)
$$

其中：

- $f_{w_i}$：词 $w_i$ 出现的频数  
- $F$：所有词总出现次数  
- $N$：文档总数  
- $N_i$：包含词 $w_i$ 的文档数  

最终公式转为 TF-IDF 形式：

$$
\sum_{w_i \in W} \sum_{d_j \in D} \frac{f_{ij}}{F} \log \frac{N}{N_i}
$$

说明 TF-IDF 实际上是互信息的一种估计。

## Word Embeddings 类型

### Frequency-based embeddings（基于频率的词向量）

- **Count (BoW)**：词袋模型，统计频次  
- **TF-IDF**：加权词频  
- **Co-occurrence**：共现矩阵，如 GloVe

### Prediction-based embeddings（基于预测的词嵌入）

- **Skip-gram**：预测上下文  
- **CBOW**：预测中心词

## GloVe

GloVe = log-bilinear regression over global co-occurrence matrix。所得到的词向量代表**全局共现统计**（global co-occurrences）。

目标思想：

$$
F\left( (w_i - w_j)^T \tilde{w}_k \right) = \frac{F(w_j^T \tilde{w}_k)}{F(w_i^T \tilde{w}_k)}
$$

即两个词向量之差 $(w_i - w_j)$ 与第三个词 $w_k$ 的上下文向量 $\tilde{w}_k$ 的点积反映了它们对 $w_k$ 共现的相对关系。  
这表示两个词对其他词的**共现比例差异决定词向量的相对位置**。

- 两个词经常一起出现 → 向量点积大  
- 几乎不一起出现 → 向量点积小，甚至为负

为什么要取对数：因为共现次数差距太大（如某些词出现几千次，某些只出现几次），使用 $\log$ 变换可以压缩数量级，使模型更稳定。

---

### GloVe 的 Cost Function

比较两个词对 (i, k) 和 (j, k) 的共现比值：

$$
\log(X_{ik}) - \log(X_{jk})
$$

其中 $X_{ik}$ 是词 $i$ 和词 $k$ 的共现次数。

训练时，将模型预测值定义为：

$$
w_i^T \tilde{w}_k + b_i + \tilde{b}_k = \log(X_{ik})
$$

- $w_i$：词 $i$ 的主词向量  
- $\tilde{w}_k$：词 $k$ 的上下文向量  
- $b_i$ 和 $\tilde{b}_k$：对应的偏置项  
- $\log(X_{ik})$：共现频数的对数

在训练过程中，词被分别当作输入词和上下文词（input/output 双向量），训练后将两者合并得到最终词向量。

### 对称性处理

由于 $\log(X_i)$ 与 $k$ 无关，我们将其定义为词 $i$ 的偏置项 $b_i$，并对称地加入词 $k$ 的偏置项 $\tilde{b}_k$，得到上式。

---

### 引入加权项（Weighted Least Squares）

为避免低频共现带来的噪声，GloVe 不直接最小化平方误差，而是使用加权函数：

$$
J = \sum_{i,j=1}^V f(X_{ij}) \left(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij} \right)^2
$$

- $f(X_{ij})$：**权重函数**，用于调节共现项对损失函数的贡献

高频词如 "the"、"of" → 权重减小  
低频词 → 可能是噪声，也减小权重  
中频词 → 贡献最大

### 权重函数 $f(x)$ 的设计要求：

- 连续（continuous）  
- 单调不减（non-decreasing）  
- $x$ 很大时 $f(x)$ 变小，防止高频词支配模型  
- $x$ 很小时也限制，避免低频词误导模型

GloVe 通常使用如下函数：

$$
f(x) =
\begin{cases}
\left( \frac{x}{x_{\text{max}}} \right)^\alpha & \text{if } x < x_{\text{max}} \\
1 & \text{otherwise}
\end{cases}
$$

其中 $x_{\text{max}}$ 和 $\alpha$ 是超参数，通常设置为 $x_{\text{max}} = 100$, $\alpha = 0.75$。

---

### 最终优化公式：

GloVe 模型学习的目标是使预测值逼近实际的 $\log$ 共现频数：

$$
w_i^T \tilde{w}_j = \log X_{ij} - b_i - \tilde{b}_j
$$


## Continuous Bag of Words (CBOW)

CBOW 使用一个固定大小的滑动窗口，以中间词（focal word）为中心，两侧的词作为“上下文（context）”，不考虑上下文词的顺序。每个词都有一个稠密向量（embedding）。

**示例**：  
句子 "The quick brown fox jumps over the lazy dog"，窗口大小为 2，  
focal word 是 "fox"，其上下文词为："quick", "brown", "jumps", "over"

输入是上下文词的 one-hot 编码：

```

quick  →  \[0, 0, 0, ..., 0, 1, 0, ..., 0]  （只有 quick 的位置是 1）
brown  →  \[0, 1, 0, ..., 0]
jumps  →  \[0, 0, 1, ..., 0]
over   →  \[0, 0, 0, ..., 1]

```

这些 one-hot 向量通过输入层（乘一个权重矩阵 $W$）变为词向量，然后对这 4 个词向量做平均：

$$
\boldsymbol{h} = \frac{1}{4} \left( \boldsymbol{v}_{\text{quick}} + \boldsymbol{v}_{\text{brown}} + \boldsymbol{v}_{\text{jumps}} + \boldsymbol{v}_{\text{over}} \right)
$$

然后用这个平均向量 $\boldsymbol{h}$ 去预测中心词 "fox"：

$$
\text{probability} = \text{softmax}(W' \cdot \boldsymbol{h})
$$

Softmax 会输出一个维度为词表大小 $V$ 的概率分布，模型希望此时 "fox" 的对应位置概率最高。

---

### CBOW 对比 Skip-Gram

| 模型      | 输入                          | 输出                                | 训练次数   |
|-----------|-------------------------------|-------------------------------------|------------|
| CBOW      | [quick, brown, jumps, over]   | 预测中心词 "fox"                    | 1 次训练   |
| Skip-Gram | 中心词 "fox"                  | 分别预测 quick, brown, jumps, over | 4 次训练   |

- **CBOW**：给定多个上下文词，预测中心词。训练样本少，适合高频词。
- **Skip-Gram**：给定中心词，预测多个上下文词。训练样本多，适合低频词。

---

### CBOW 模型的数学计算

每个输出词的得分 $u_j$ 是隐藏层向量与输出词向量的点积：

$$
u_j = \boldsymbol{v}_{w_j}^T \cdot \boldsymbol{h}
$$

- $\boldsymbol{v}_{w_j}$：词 $w_j$ 的输出向量  
- $\boldsymbol{h}$：隐藏层向量（上下文平均向量）

通过 Softmax 得到词 $w_j$ 被预测为输出词的概率：

$$
p(w_j | w_I) = y_j = \frac{ \exp\left( \boldsymbol{v}_{w_j}^T \boldsymbol{v}_{w_I} \right)}{ \sum_{j'=1}^{V} \exp\left( \boldsymbol{v}_{w_{j'}}^T \boldsymbol{v}_{w_I} \right)}
$$

- $\boldsymbol{v}_{w_I}$：上下文词向量的平均值  
- $\boldsymbol{v}_{w_j}$：候选输出词向量

这是一个 softmax 分类器，用于最大化正确词的预测概率。

---

### CBOW 的目标函数

目标是最大化给定上下文词 $w_I$ 时，输出目标词 $w_o$ 的概率：

$$
\max p(w_o \mid w_I) = \max y_j = \max \log y_j = u_j^* - \log \sum_{j'} \exp(u_{j'})
$$

这对应的是**交叉熵损失（cross-entropy loss）**：

$$
-\log p(w_o \mid w_I)
$$

等价于负对数似然（Negative Log Likelihood, NLL）

---

### CBOW 中的两个矩阵：

- $W$：输入词向量矩阵（input $\rightarrow$ hidden）  
- $W'$：输出词向量矩阵（hidden $\rightarrow$ output）


## Skip-Gram 

* 改进 Skip-gram 模型的方法，用来提高 embedding 的质量和训练速度
* 将短语看作一个词来 embedding
* 展示了如何通过向量运算捕捉语义和语法，以及类比推理
* 强调数据量重要

（参见：**Mikolov et al., 2013**）

---

### 背景

* **Distributed Word Vector Representation** 的核心思想是：**词的意义由它的上下文决定**
* 模型通过学习一个词在不同语境中出现的方式来 embedding。让“用法相似”的词拥有“空间上接近”的向量，使机器具备了基本的语义理解能力
* Skip-gram 模型是一种 Distributed Word Vector Representation，比如 Word2Vec
* 目标是：给定一个词，预测它周围出现的词（上下文）

---

### 模型结构概览

* 从大量非结构化文本中学习 embedding
* 早期的 NN 模型结构：输入层 → 投影层（word embedding lookup）→ 隐藏层（全连接）→ softmax 输出层
* 大量使用全连接层，涉及大规模矩阵乘法，特别是 softmax 层需要对整个词表做归一化，代价是 O(V)，非常昂贵
* Skip-gram 不需要隐藏层，每次训练只涉及两个向量的 dot product，因此效率高
* CBOW 则是用多个上下文词的平均向量预测中心词（更快但不如 Skip-gram 细致）

---

### 数学机制：Skip-Gram 与 CBOW

#### Skip-Gram 数学计算

* 输入是中心词 $w_I$，目标是每个上下文词 $w_O$
* 模型通过最大化：

$$
p(w_O | w_I) = \frac{\exp(v_{w_O}^T \cdot v_{w_I})}{\sum_{w' \in V} \exp(v_{w'}^T \cdot v_{w_I})}
$$

* 损失函数为：

$$
L = - \log p(w_O | w_I)
$$

* 每个训练样本对应一个这样的二分类预测（中心词 → 一个上下文词）

---

### 词向量的结构性语义（Additive Compositionality）

* 许多语言规律和模式可以通过线性转换来表示：

$$
\text{vec}("King") - \text{vec}("Man") + \text{vec}("Woman") ≈ \text{vec}("Queen")
$$

* 向量差 → 表示语法/语义关系（如性别、时态、国家 vs 首都）
* 可以进行 analogy reasoning（类比推理）

---

### Skip-gram 模型扩展和优化方法

#### Hierarchical Softmax

* 原始 softmax 的分母是整个词表，复杂度 O(V)
* Hierarchical Softmax 用 Huffman 树，把 softmax 转换为走一棵二叉树的路径
* 代价从 O(V) 降为 O(log V)
* 高频词路径更短，更新更快

#### Negative Sampling

* 将多分类 softmax 问题转化为多个二分类任务
* 训练目标：让模型区分“正样本词（上下文）”和“噪声词”
* 损失函数：

$$
\log \sigma(v_{w_o}^T h) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)}[\log \sigma(-v_{w_i}^T h)]
$$

* 只更新 1 个正样本 + k 个负样本，效率极高

#### Subsampling of Frequent Words

* 高频词如 "the"、"is" 太常见但信息少，可随机丢弃一部分：

$$
P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}
$$

* 通常设 $t = 10^{-5}$
* 能大幅提升训练速度并提高稀有词表示质量

#### Learning Phrases

* 用统计方法识别出高共现且语义连贯的词对（如“New\_York”）
* 将其作为一个 token 纳入训练
* 增强模型表示能力、支持短语类比推理

---

### 实验与结果

* Negative Sampling 通常效果更好，尤其在训练速度和稀有词表示方面
* 负样本数 $k$ 增大（例如 5 → 15）会提升效果
* Subsampling 提高了训练速度和准确性（2–10 倍）
* 能学出有意义、可解释的词向量结构

